{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read2.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read2.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = keras.utils.to_categorical(label, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1513/1513 [==============================] - 0s 246us/step - loss: 4.7100 - acc: 0.2558\n",
      "Epoch 2/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 1.9825 - acc: 0.6021\n",
      "Epoch 3/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 1.6789 - acc: 0.6728\n",
      "Epoch 4/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 1.4610 - acc: 0.7085\n",
      "Epoch 5/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 1.2975 - acc: 0.7191\n",
      "Epoch 6/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 1.1929 - acc: 0.7422\n",
      "Epoch 7/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 1.1132 - acc: 0.7462\n",
      "Epoch 8/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 1.0387 - acc: 0.7594\n",
      "Epoch 9/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.9901 - acc: 0.7627\n",
      "Epoch 10/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.9343 - acc: 0.7687\n",
      "Epoch 11/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.9093 - acc: 0.7713\n",
      "Epoch 12/200\n",
      "1513/1513 [==============================] - ETA: 0s - loss: 0.8195 - acc: 0.802 - 0s 59us/step - loss: 0.8608 - acc: 0.7845\n",
      "Epoch 13/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.8395 - acc: 0.7845\n",
      "Epoch 14/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.8141 - acc: 0.7892\n",
      "Epoch 15/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.7834 - acc: 0.7944\n",
      "Epoch 16/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.7710 - acc: 0.7958\n",
      "Epoch 17/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.7543 - acc: 0.8083\n",
      "Epoch 18/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.7277 - acc: 0.8110\n",
      "Epoch 19/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.7199 - acc: 0.8156\n",
      "Epoch 20/200\n",
      "1513/1513 [==============================] - 0s 82us/step - loss: 0.7059 - acc: 0.8123\n",
      "Epoch 21/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.7019 - acc: 0.8110\n",
      "Epoch 22/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.6721 - acc: 0.8209\n",
      "Epoch 23/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.6736 - acc: 0.8176: 0s - loss: 0.6771 - acc: 0.815\n",
      "Epoch 24/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.6699 - acc: 0.8215\n",
      "Epoch 25/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.6682 - acc: 0.8163\n",
      "Epoch 26/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.6456 - acc: 0.8196\n",
      "Epoch 27/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.6503 - acc: 0.8242\n",
      "Epoch 28/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.6223 - acc: 0.8242\n",
      "Epoch 29/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.6425 - acc: 0.8196\n",
      "Epoch 30/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.6296 - acc: 0.8235\n",
      "Epoch 31/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.6161 - acc: 0.8354\n",
      "Epoch 32/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.6176 - acc: 0.8295\n",
      "Epoch 33/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.6123 - acc: 0.8374\n",
      "Epoch 34/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.6106 - acc: 0.8341\n",
      "Epoch 35/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.6003 - acc: 0.8301\n",
      "Epoch 36/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.5986 - acc: 0.8308\n",
      "Epoch 37/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.6003 - acc: 0.8321\n",
      "Epoch 38/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.6008 - acc: 0.8334\n",
      "Epoch 39/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.5777 - acc: 0.8387\n",
      "Epoch 40/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.5875 - acc: 0.8381\n",
      "Epoch 41/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.5843 - acc: 0.8407\n",
      "Epoch 42/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.5707 - acc: 0.8460\n",
      "Epoch 43/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.5786 - acc: 0.8394\n",
      "Epoch 44/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.5692 - acc: 0.8473\n",
      "Epoch 45/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.5723 - acc: 0.8381\n",
      "Epoch 46/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.5639 - acc: 0.8519\n",
      "Epoch 47/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.5570 - acc: 0.8473\n",
      "Epoch 48/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.5586 - acc: 0.8486\n",
      "Epoch 49/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.5557 - acc: 0.8460\n",
      "Epoch 50/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.5513 - acc: 0.8453\n",
      "Epoch 51/200\n",
      "1513/1513 [==============================] - ETA: 0s - loss: 0.5609 - acc: 0.844 - 0s 60us/step - loss: 0.5481 - acc: 0.8486\n",
      "Epoch 52/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.5428 - acc: 0.8493\n",
      "Epoch 53/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.5413 - acc: 0.8453\n",
      "Epoch 54/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.5413 - acc: 0.8486\n",
      "Epoch 55/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.5285 - acc: 0.8493\n",
      "Epoch 56/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.5327 - acc: 0.8453\n",
      "Epoch 57/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.5249 - acc: 0.8533\n",
      "Epoch 58/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.5260 - acc: 0.8539\n",
      "Epoch 59/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.5231 - acc: 0.8480\n",
      "Epoch 60/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.5319 - acc: 0.8519\n",
      "Epoch 61/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.5209 - acc: 0.8513\n",
      "Epoch 62/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.5130 - acc: 0.8559\n",
      "Epoch 63/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.5156 - acc: 0.8566\n",
      "Epoch 64/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.5096 - acc: 0.8612\n",
      "Epoch 65/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.5189 - acc: 0.8579\n",
      "Epoch 66/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.4971 - acc: 0.8579\n",
      "Epoch 67/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.5056 - acc: 0.8592\n",
      "Epoch 68/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.5020 - acc: 0.8599\n",
      "Epoch 69/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.5007 - acc: 0.8566\n",
      "Epoch 70/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.4979 - acc: 0.8553\n",
      "Epoch 71/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.4924 - acc: 0.8599\n",
      "Epoch 72/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.4908 - acc: 0.8672\n",
      "Epoch 73/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.4933 - acc: 0.8599\n",
      "Epoch 74/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.4899 - acc: 0.8599\n",
      "Epoch 75/200\n",
      "1513/1513 [==============================] - 0s 72us/step - loss: 0.4938 - acc: 0.8619: 0s - loss: 0.4837 - acc: 0.86\n",
      "Epoch 76/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.4879 - acc: 0.8605\n",
      "Epoch 77/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.4813 - acc: 0.8638\n",
      "Epoch 78/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4780 - acc: 0.8672\n",
      "Epoch 79/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.4918 - acc: 0.8645\n",
      "Epoch 80/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.4740 - acc: 0.8731\n",
      "Epoch 81/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.4804 - acc: 0.8672\n",
      "Epoch 82/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4822 - acc: 0.8619\n",
      "Epoch 83/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.4761 - acc: 0.8612\n",
      "Epoch 84/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.4753 - acc: 0.8685\n",
      "Epoch 85/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.4680 - acc: 0.8698\n",
      "Epoch 86/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.4655 - acc: 0.8672\n",
      "Epoch 87/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.4676 - acc: 0.8711\n",
      "Epoch 88/200\n",
      "1513/1513 [==============================] - 0s 68us/step - loss: 0.4683 - acc: 0.8685\n",
      "Epoch 89/200\n",
      "1513/1513 [==============================] - 0s 75us/step - loss: 0.4648 - acc: 0.8764\n",
      "Epoch 90/200\n",
      "1513/1513 [==============================] - 0s 79us/step - loss: 0.4699 - acc: 0.8705\n",
      "Epoch 91/200\n",
      "1513/1513 [==============================] - 0s 78us/step - loss: 0.4571 - acc: 0.8711\n",
      "Epoch 92/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.4666 - acc: 0.8665\n",
      "Epoch 93/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.4666 - acc: 0.8691\n",
      "Epoch 94/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.4553 - acc: 0.8711\n",
      "Epoch 95/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.4550 - acc: 0.8685\n",
      "Epoch 96/200\n",
      "1513/1513 [==============================] - 0s 101us/step - loss: 0.4504 - acc: 0.8784\n",
      "Epoch 97/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.4612 - acc: 0.8658\n",
      "Epoch 98/200\n",
      "1513/1513 [==============================] - 0s 78us/step - loss: 0.4533 - acc: 0.8738\n",
      "Epoch 99/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.4529 - acc: 0.8705\n",
      "Epoch 100/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.4455 - acc: 0.8744\n",
      "Epoch 101/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.4575 - acc: 0.8678\n",
      "Epoch 102/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4501 - acc: 0.8771\n",
      "Epoch 103/200\n",
      "1513/1513 [==============================] - 0s 90us/step - loss: 0.4494 - acc: 0.8738\n",
      "Epoch 104/200\n",
      "1513/1513 [==============================] - 0s 78us/step - loss: 0.4424 - acc: 0.8810\n",
      "Epoch 105/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.4381 - acc: 0.8790\n",
      "Epoch 106/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.4431 - acc: 0.8764\n",
      "Epoch 107/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.4399 - acc: 0.8830\n",
      "Epoch 108/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.4365 - acc: 0.8797\n",
      "Epoch 109/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.4405 - acc: 0.8784\n",
      "Epoch 110/200\n",
      "1513/1513 [==============================] - 0s 70us/step - loss: 0.4276 - acc: 0.8764\n",
      "Epoch 111/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.4286 - acc: 0.8804\n",
      "Epoch 112/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4373 - acc: 0.8810\n",
      "Epoch 113/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.4355 - acc: 0.8784\n",
      "Epoch 114/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.4374 - acc: 0.8837\n",
      "Epoch 115/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.4230 - acc: 0.8850\n",
      "Epoch 116/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.4362 - acc: 0.8810\n",
      "Epoch 117/200\n",
      "1513/1513 [==============================] - 0s 70us/step - loss: 0.4218 - acc: 0.8810\n",
      "Epoch 118/200\n",
      "1513/1513 [==============================] - 0s 79us/step - loss: 0.4324 - acc: 0.8863\n",
      "Epoch 119/200\n",
      "1513/1513 [==============================] - 0s 85us/step - loss: 0.4277 - acc: 0.8830\n",
      "Epoch 120/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.4312 - acc: 0.8857\n",
      "Epoch 121/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.4192 - acc: 0.8890\n",
      "Epoch 122/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.4211 - acc: 0.8896\n",
      "Epoch 123/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.4238 - acc: 0.8797\n",
      "Epoch 124/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.4233 - acc: 0.8863\n",
      "Epoch 125/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.4198 - acc: 0.8857\n",
      "Epoch 126/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.4134 - acc: 0.8896\n",
      "Epoch 127/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.4210 - acc: 0.8929\n",
      "Epoch 128/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4237 - acc: 0.8863\n",
      "Epoch 129/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.4196 - acc: 0.8909\n",
      "Epoch 130/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.4128 - acc: 0.8916\n",
      "Epoch 131/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.4070 - acc: 0.8896\n",
      "Epoch 132/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3950 - acc: 0.8909\n",
      "Epoch 133/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.4058 - acc: 0.8896\n",
      "Epoch 134/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3980 - acc: 0.8942\n",
      "Epoch 135/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.4008 - acc: 0.8883\n",
      "Epoch 136/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3971 - acc: 0.8969\n",
      "Epoch 137/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3974 - acc: 0.8896\n",
      "Epoch 138/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3990 - acc: 0.8956\n",
      "Epoch 139/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.4009 - acc: 0.8949\n",
      "Epoch 140/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.4059 - acc: 0.8923\n",
      "Epoch 141/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3917 - acc: 0.8903\n",
      "Epoch 142/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.4019 - acc: 0.8949\n",
      "Epoch 143/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3918 - acc: 0.8936\n",
      "Epoch 144/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3983 - acc: 0.8896\n",
      "Epoch 145/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3928 - acc: 0.8969\n",
      "Epoch 146/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.3779 - acc: 0.8949\n",
      "Epoch 147/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3876 - acc: 0.8936\n",
      "Epoch 148/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.3878 - acc: 0.8923\n",
      "Epoch 149/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3961 - acc: 0.8923\n",
      "Epoch 150/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3821 - acc: 0.8936\n",
      "Epoch 151/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3921 - acc: 0.8942\n",
      "Epoch 152/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.3852 - acc: 0.8949\n",
      "Epoch 153/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3851 - acc: 0.9002\n",
      "Epoch 154/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3806 - acc: 0.8969\n",
      "Epoch 155/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3861 - acc: 0.8956\n",
      "Epoch 156/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3871 - acc: 0.8903\n",
      "Epoch 157/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.3735 - acc: 0.8969\n",
      "Epoch 158/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3697 - acc: 0.8962\n",
      "Epoch 159/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3696 - acc: 0.8989\n",
      "Epoch 160/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3694 - acc: 0.8976\n",
      "Epoch 161/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.3701 - acc: 0.9022\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.3706 - acc: 0.9022\n",
      "Epoch 163/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3685 - acc: 0.8949\n",
      "Epoch 164/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3607 - acc: 0.9015\n",
      "Epoch 165/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.3709 - acc: 0.9002\n",
      "Epoch 166/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3664 - acc: 0.9035\n",
      "Epoch 167/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3642 - acc: 0.9015\n",
      "Epoch 168/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3609 - acc: 0.9028\n",
      "Epoch 169/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3557 - acc: 0.9009\n",
      "Epoch 170/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3649 - acc: 0.9002\n",
      "Epoch 171/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.3684 - acc: 0.9035\n",
      "Epoch 172/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.3599 - acc: 0.9028\n",
      "Epoch 173/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.3635 - acc: 0.8995\n",
      "Epoch 174/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3605 - acc: 0.9015\n",
      "Epoch 175/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.3687 - acc: 0.8995\n",
      "Epoch 176/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3595 - acc: 0.9022\n",
      "Epoch 177/200\n",
      "1513/1513 [==============================] - 0s 83us/step - loss: 0.3584 - acc: 0.9035\n",
      "Epoch 178/200\n",
      "1513/1513 [==============================] - 0s 86us/step - loss: 0.3600 - acc: 0.9022\n",
      "Epoch 179/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.3417 - acc: 0.9075\n",
      "Epoch 180/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.3443 - acc: 0.9075\n",
      "Epoch 181/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.3427 - acc: 0.9028\n",
      "Epoch 182/200\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.3411 - acc: 0.9015\n",
      "Epoch 183/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.3455 - acc: 0.9055\n",
      "Epoch 184/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.3447 - acc: 0.9075\n",
      "Epoch 185/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.3472 - acc: 0.9081\n",
      "Epoch 186/200\n",
      "1513/1513 [==============================] - 0s 74us/step - loss: 0.3453 - acc: 0.9114\n",
      "Epoch 187/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3467 - acc: 0.9081\n",
      "Epoch 188/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.3456 - acc: 0.9015\n",
      "Epoch 189/200\n",
      "1513/1513 [==============================] - 0s 85us/step - loss: 0.3500 - acc: 0.8962\n",
      "Epoch 190/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.3352 - acc: 0.9114\n",
      "Epoch 191/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3464 - acc: 0.9028\n",
      "Epoch 192/200\n",
      "1513/1513 [==============================] - 0s 83us/step - loss: 0.3351 - acc: 0.9075\n",
      "Epoch 193/200\n",
      "1513/1513 [==============================] - 0s 91us/step - loss: 0.3508 - acc: 0.9022\n",
      "Epoch 194/200\n",
      "1513/1513 [==============================] - 0s 70us/step - loss: 0.3366 - acc: 0.9088\n",
      "Epoch 195/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.3382 - acc: 0.9068\n",
      "Epoch 196/200\n",
      "1513/1513 [==============================] - 0s 96us/step - loss: 0.3412 - acc: 0.9048\n",
      "Epoch 197/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3437 - acc: 0.9048\n",
      "Epoch 198/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.3377 - acc: 0.9028\n",
      "Epoch 199/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.3437 - acc: 0.9022\n",
      "Epoch 200/200\n",
      "1513/1513 [==============================] - 0s 79us/step - loss: 0.3335 - acc: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b92eea20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.3317 - acc: 0.9048\n",
      "Epoch 2/50\n",
      "1513/1513 [==============================] - 0s 44us/step - loss: 0.3414 - acc: 0.9035\n",
      "Epoch 3/50\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3385 - acc: 0.9061\n",
      "Epoch 4/50\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3365 - acc: 0.9075\n",
      "Epoch 5/50\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.3364 - acc: 0.9075\n",
      "Epoch 6/50\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3414 - acc: 0.9068\n",
      "Epoch 7/50\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3453 - acc: 0.9022\n",
      "Epoch 8/50\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3342 - acc: 0.9121\n",
      "Epoch 9/50\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3364 - acc: 0.9068\n",
      "Epoch 10/50\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3369 - acc: 0.8982\n",
      "Epoch 11/50\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3324 - acc: 0.9075\n",
      "Epoch 12/50\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.3351 - acc: 0.9061\n",
      "Epoch 13/50\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3275 - acc: 0.9068\n",
      "Epoch 14/50\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.3315 - acc: 0.9055\n",
      "Epoch 15/50\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.3419 - acc: 0.9042\n",
      "Epoch 16/50\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3283 - acc: 0.9075\n",
      "Epoch 17/50\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.3334 - acc: 0.9095\n",
      "Epoch 18/50\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3266 - acc: 0.9088\n",
      "Epoch 19/50\n",
      "1513/1513 [==============================] - 0s 83us/step - loss: 0.3252 - acc: 0.9081\n",
      "Epoch 20/50\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.3343 - acc: 0.9095\n",
      "Epoch 21/50\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.3329 - acc: 0.9081\n",
      "Epoch 22/50\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.3253 - acc: 0.9068\n",
      "Epoch 23/50\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3307 - acc: 0.9095\n",
      "Epoch 24/50\n",
      "1513/1513 [==============================] - 0s 86us/step - loss: 0.3274 - acc: 0.9095\n",
      "Epoch 25/50\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.3322 - acc: 0.9042\n",
      "Epoch 26/50\n",
      "1513/1513 [==============================] - 0s 87us/step - loss: 0.3260 - acc: 0.9101\n",
      "Epoch 27/50\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3269 - acc: 0.9114\n",
      "Epoch 28/50\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.3325 - acc: 0.9101\n",
      "Epoch 29/50\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.3261 - acc: 0.9095\n",
      "Epoch 30/50\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.3297 - acc: 0.9108\n",
      "Epoch 31/50\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3358 - acc: 0.9061\n",
      "Epoch 32/50\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3228 - acc: 0.9088\n",
      "Epoch 33/50\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.3239 - acc: 0.9128\n",
      "Epoch 34/50\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.3319 - acc: 0.9134\n",
      "Epoch 35/50\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3261 - acc: 0.9068\n",
      "Epoch 36/50\n",
      "1513/1513 [==============================] - 0s 113us/step - loss: 0.3273 - acc: 0.9101\n",
      "Epoch 37/50\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.3318 - acc: 0.9055\n",
      "Epoch 38/50\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.3278 - acc: 0.9095\n",
      "Epoch 39/50\n",
      "1513/1513 [==============================] - 0s 92us/step - loss: 0.3272 - acc: 0.9095\n",
      "Epoch 40/50\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.3173 - acc: 0.9187\n",
      "Epoch 41/50\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3255 - acc: 0.9068\n",
      "Epoch 42/50\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.3213 - acc: 0.9161\n",
      "Epoch 43/50\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3292 - acc: 0.9114\n",
      "Epoch 44/50\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.3253 - acc: 0.9095\n",
      "Epoch 45/50\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3211 - acc: 0.9114\n",
      "Epoch 46/50\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3155 - acc: 0.9128\n",
      "Epoch 47/50\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3188 - acc: 0.9128\n",
      "Epoch 48/50\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3260 - acc: 0.9081\n",
      "Epoch 49/50\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3109 - acc: 0.9128\n",
      "Epoch 50/50\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3198 - acc: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b92eeb70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.3147 - acc: 0.9121\n",
      "Epoch 2/200\n",
      "1513/1513 [==============================] - 0s 42us/step - loss: 0.3275 - acc: 0.9068\n",
      "Epoch 3/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3278 - acc: 0.9095\n",
      "Epoch 4/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3184 - acc: 0.9101\n",
      "Epoch 5/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.3158 - acc: 0.9088\n",
      "Epoch 6/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3094 - acc: 0.9121\n",
      "Epoch 7/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3234 - acc: 0.9068\n",
      "Epoch 8/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3195 - acc: 0.9134\n",
      "Epoch 9/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.3170 - acc: 0.9095\n",
      "Epoch 10/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3160 - acc: 0.9187\n",
      "Epoch 11/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3167 - acc: 0.9114\n",
      "Epoch 12/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3140 - acc: 0.9141\n",
      "Epoch 13/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3130 - acc: 0.9114\n",
      "Epoch 14/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3152 - acc: 0.9161\n",
      "Epoch 15/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3229 - acc: 0.9108\n",
      "Epoch 16/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.3032 - acc: 0.9200\n",
      "Epoch 17/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3141 - acc: 0.9147\n",
      "Epoch 18/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.3098 - acc: 0.9128\n",
      "Epoch 19/200\n",
      "1513/1513 [==============================] - 0s 68us/step - loss: 0.3102 - acc: 0.9134\n",
      "Epoch 20/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3107 - acc: 0.9114\n",
      "Epoch 21/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3145 - acc: 0.9114\n",
      "Epoch 22/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3149 - acc: 0.9141\n",
      "Epoch 23/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.3089 - acc: 0.9147\n",
      "Epoch 24/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.3075 - acc: 0.9213\n",
      "Epoch 25/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3055 - acc: 0.9154\n",
      "Epoch 26/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3085 - acc: 0.9128\n",
      "Epoch 27/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3047 - acc: 0.9134\n",
      "Epoch 28/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3117 - acc: 0.9141\n",
      "Epoch 29/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3039 - acc: 0.9161\n",
      "Epoch 30/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.3050 - acc: 0.9114\n",
      "Epoch 31/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3048 - acc: 0.9101\n",
      "Epoch 32/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.3014 - acc: 0.9147\n",
      "Epoch 33/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.3087 - acc: 0.9141\n",
      "Epoch 34/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.3020 - acc: 0.9174\n",
      "Epoch 35/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3063 - acc: 0.9108\n",
      "Epoch 36/200\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.3070 - acc: 0.9161\n",
      "Epoch 37/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.3072 - acc: 0.9121\n",
      "Epoch 38/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2986 - acc: 0.9194\n",
      "Epoch 39/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2942 - acc: 0.9220\n",
      "Epoch 40/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.3084 - acc: 0.9154\n",
      "Epoch 41/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2994 - acc: 0.9187\n",
      "Epoch 42/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.3084 - acc: 0.9114\n",
      "Epoch 43/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.3001 - acc: 0.9141\n",
      "Epoch 44/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.3021 - acc: 0.9161\n",
      "Epoch 45/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2999 - acc: 0.9134\n",
      "Epoch 46/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.2981 - acc: 0.9134\n",
      "Epoch 47/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2979 - acc: 0.9180\n",
      "Epoch 48/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3061 - acc: 0.9200\n",
      "Epoch 49/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2935 - acc: 0.9194\n",
      "Epoch 50/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.3071 - acc: 0.9200\n",
      "Epoch 51/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2977 - acc: 0.9200\n",
      "Epoch 52/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.3010 - acc: 0.9128\n",
      "Epoch 53/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2996 - acc: 0.9213\n",
      "Epoch 54/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2946 - acc: 0.9167\n",
      "Epoch 55/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2904 - acc: 0.9180\n",
      "Epoch 56/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2993 - acc: 0.9180\n",
      "Epoch 57/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.3007 - acc: 0.9180\n",
      "Epoch 58/200\n",
      "1513/1513 [==============================] - 0s 68us/step - loss: 0.3027 - acc: 0.9180\n",
      "Epoch 59/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2960 - acc: 0.9167\n",
      "Epoch 60/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.3001 - acc: 0.9141\n",
      "Epoch 61/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2970 - acc: 0.9187\n",
      "Epoch 62/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2981 - acc: 0.9154\n",
      "Epoch 63/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2914 - acc: 0.9220\n",
      "Epoch 64/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2933 - acc: 0.9174\n",
      "Epoch 65/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2965 - acc: 0.9240\n",
      "Epoch 66/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2927 - acc: 0.9220\n",
      "Epoch 67/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2901 - acc: 0.9194\n",
      "Epoch 68/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2967 - acc: 0.9187\n",
      "Epoch 69/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2985 - acc: 0.9128\n",
      "Epoch 70/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2917 - acc: 0.9161\n",
      "Epoch 71/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2998 - acc: 0.9147\n",
      "Epoch 72/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2907 - acc: 0.9200\n",
      "Epoch 73/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2950 - acc: 0.9161\n",
      "Epoch 74/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2955 - acc: 0.9187\n",
      "Epoch 75/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2959 - acc: 0.9180\n",
      "Epoch 76/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2912 - acc: 0.9180\n",
      "Epoch 77/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2982 - acc: 0.9213\n",
      "Epoch 78/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2929 - acc: 0.9167\n",
      "Epoch 79/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2957 - acc: 0.9154\n",
      "Epoch 80/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2872 - acc: 0.9207\n",
      "Epoch 81/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.3033 - acc: 0.9134\n",
      "Epoch 82/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2889 - acc: 0.9194\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2950 - acc: 0.9194\n",
      "Epoch 84/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2869 - acc: 0.9194\n",
      "Epoch 85/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2931 - acc: 0.9174\n",
      "Epoch 86/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2919 - acc: 0.9187\n",
      "Epoch 87/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2904 - acc: 0.9161\n",
      "Epoch 88/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2849 - acc: 0.9220\n",
      "Epoch 89/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.2913 - acc: 0.9240\n",
      "Epoch 90/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2937 - acc: 0.9220\n",
      "Epoch 91/200\n",
      "1513/1513 [==============================] - 0s 102us/step - loss: 0.2926 - acc: 0.9220\n",
      "Epoch 92/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2935 - acc: 0.9114\n",
      "Epoch 93/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2870 - acc: 0.9194\n",
      "Epoch 94/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2858 - acc: 0.9213\n",
      "Epoch 95/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.2852 - acc: 0.9207\n",
      "Epoch 96/200\n",
      "1513/1513 [==============================] - 0s 119us/step - loss: 0.2917 - acc: 0.9200\n",
      "Epoch 97/200\n",
      "1513/1513 [==============================] - 0s 71us/step - loss: 0.2916 - acc: 0.9260\n",
      "Epoch 98/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2874 - acc: 0.9220\n",
      "Epoch 99/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2883 - acc: 0.9180\n",
      "Epoch 100/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.2880 - acc: 0.9233\n",
      "Epoch 101/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.2911 - acc: 0.9233\n",
      "Epoch 102/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2839 - acc: 0.9227\n",
      "Epoch 103/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2823 - acc: 0.9260\n",
      "Epoch 104/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2872 - acc: 0.9213\n",
      "Epoch 105/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2824 - acc: 0.9227\n",
      "Epoch 106/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2860 - acc: 0.9194\n",
      "Epoch 107/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2841 - acc: 0.9233\n",
      "Epoch 108/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2903 - acc: 0.9200\n",
      "Epoch 109/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2852 - acc: 0.9253\n",
      "Epoch 110/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2812 - acc: 0.9213\n",
      "Epoch 111/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2827 - acc: 0.9233\n",
      "Epoch 112/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2903 - acc: 0.9180\n",
      "Epoch 113/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2862 - acc: 0.9240\n",
      "Epoch 114/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2820 - acc: 0.9167\n",
      "Epoch 115/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.2885 - acc: 0.9187\n",
      "Epoch 116/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2769 - acc: 0.9253\n",
      "Epoch 117/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2789 - acc: 0.9233\n",
      "Epoch 118/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2818 - acc: 0.9187\n",
      "Epoch 119/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2778 - acc: 0.9299\n",
      "Epoch 120/200\n",
      "1513/1513 [==============================] - 0s 44us/step - loss: 0.2842 - acc: 0.9187\n",
      "Epoch 121/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2849 - acc: 0.9247\n",
      "Epoch 122/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2812 - acc: 0.9240\n",
      "Epoch 123/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.2827 - acc: 0.9220\n",
      "Epoch 124/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2776 - acc: 0.9266\n",
      "Epoch 125/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2855 - acc: 0.9240\n",
      "Epoch 126/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2805 - acc: 0.9220\n",
      "Epoch 127/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2811 - acc: 0.9227\n",
      "Epoch 128/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.2805 - acc: 0.9240\n",
      "Epoch 129/200\n",
      "1513/1513 [==============================] - 0s 77us/step - loss: 0.2797 - acc: 0.9253\n",
      "Epoch 130/200\n",
      "1513/1513 [==============================] - 0s 72us/step - loss: 0.2811 - acc: 0.9227\n",
      "Epoch 131/200\n",
      "1513/1513 [==============================] - 0s 93us/step - loss: 0.2777 - acc: 0.9213\n",
      "Epoch 132/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2784 - acc: 0.9273\n",
      "Epoch 133/200\n",
      "1513/1513 [==============================] - 0s 79us/step - loss: 0.2902 - acc: 0.9220\n",
      "Epoch 134/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2859 - acc: 0.9227\n",
      "Epoch 135/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2778 - acc: 0.9233\n",
      "Epoch 136/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2856 - acc: 0.9253\n",
      "Epoch 137/200\n",
      "1513/1513 [==============================] - 0s 78us/step - loss: 0.2783 - acc: 0.9273\n",
      "Epoch 138/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2823 - acc: 0.9213\n",
      "Epoch 139/200\n",
      "1513/1513 [==============================] - 0s 82us/step - loss: 0.2755 - acc: 0.9247\n",
      "Epoch 140/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2767 - acc: 0.9233\n",
      "Epoch 141/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2849 - acc: 0.9213\n",
      "Epoch 142/200\n",
      "1513/1513 [==============================] - 0s 78us/step - loss: 0.2797 - acc: 0.9273\n",
      "Epoch 143/200\n",
      "1513/1513 [==============================] - 0s 104us/step - loss: 0.2744 - acc: 0.9213\n",
      "Epoch 144/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2791 - acc: 0.9227\n",
      "Epoch 145/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.2721 - acc: 0.9273\n",
      "Epoch 146/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2736 - acc: 0.9273\n",
      "Epoch 147/200\n",
      "1513/1513 [==============================] - 0s 105us/step - loss: 0.2848 - acc: 0.9220\n",
      "Epoch 148/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2771 - acc: 0.9227\n",
      "Epoch 149/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2769 - acc: 0.9260\n",
      "Epoch 150/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2792 - acc: 0.9174\n",
      "Epoch 151/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2810 - acc: 0.9240\n",
      "Epoch 152/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2732 - acc: 0.9266\n",
      "Epoch 153/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2719 - acc: 0.9247\n",
      "Epoch 154/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2768 - acc: 0.9266\n",
      "Epoch 155/200\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.2767 - acc: 0.9227\n",
      "Epoch 156/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2720 - acc: 0.9260\n",
      "Epoch 157/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2738 - acc: 0.9286\n",
      "Epoch 158/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2801 - acc: 0.9187\n",
      "Epoch 159/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2778 - acc: 0.9233\n",
      "Epoch 160/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2668 - acc: 0.9273\n",
      "Epoch 161/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2694 - acc: 0.9273\n",
      "Epoch 162/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2773 - acc: 0.9233\n",
      "Epoch 163/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2696 - acc: 0.9293\n",
      "Epoch 164/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2744 - acc: 0.9253\n",
      "Epoch 165/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2740 - acc: 0.9213\n",
      "Epoch 166/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2684 - acc: 0.9273\n",
      "Epoch 167/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2786 - acc: 0.9253\n",
      "Epoch 168/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2736 - acc: 0.9313\n",
      "Epoch 169/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2738 - acc: 0.9247\n",
      "Epoch 170/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.2670 - acc: 0.9313\n",
      "Epoch 171/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.2717 - acc: 0.9273\n",
      "Epoch 172/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2769 - acc: 0.9253\n",
      "Epoch 173/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2701 - acc: 0.9273\n",
      "Epoch 174/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2751 - acc: 0.9260\n",
      "Epoch 175/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2723 - acc: 0.9286\n",
      "Epoch 176/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2695 - acc: 0.9253\n",
      "Epoch 177/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2684 - acc: 0.9280\n",
      "Epoch 178/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2732 - acc: 0.9247\n",
      "Epoch 179/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2695 - acc: 0.9280\n",
      "Epoch 180/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.2698 - acc: 0.9286\n",
      "Epoch 181/200\n",
      "1513/1513 [==============================] - 0s 84us/step - loss: 0.2639 - acc: 0.9299\n",
      "Epoch 182/200\n",
      "1513/1513 [==============================] - 0s 79us/step - loss: 0.2682 - acc: 0.9293\n",
      "Epoch 183/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2704 - acc: 0.9299\n",
      "Epoch 184/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.2712 - acc: 0.9273\n",
      "Epoch 185/200\n",
      "1513/1513 [==============================] - 0s 85us/step - loss: 0.2678 - acc: 0.9273\n",
      "Epoch 186/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.2735 - acc: 0.9247\n",
      "Epoch 187/200\n",
      "1513/1513 [==============================] - 0s 98us/step - loss: 0.2679 - acc: 0.9286\n",
      "Epoch 188/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2728 - acc: 0.9280\n",
      "Epoch 189/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2634 - acc: 0.9273\n",
      "Epoch 190/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2650 - acc: 0.9326\n",
      "Epoch 191/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2773 - acc: 0.9260\n",
      "Epoch 192/200\n",
      "1513/1513 [==============================] - 0s 75us/step - loss: 0.2642 - acc: 0.9293\n",
      "Epoch 193/200\n",
      "1513/1513 [==============================] - 0s 42us/step - loss: 0.2623 - acc: 0.9339\n",
      "Epoch 194/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2681 - acc: 0.9313\n",
      "Epoch 195/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2722 - acc: 0.9313\n",
      "Epoch 196/200\n",
      "1513/1513 [==============================] - 0s 98us/step - loss: 0.2626 - acc: 0.9339\n",
      "Epoch 197/200\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.2671 - acc: 0.9280\n",
      "Epoch 198/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2663 - acc: 0.9273\n",
      "Epoch 199/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2787 - acc: 0.9240\n",
      "Epoch 200/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2644 - acc: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b92eeba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2587 - acc: 0.9372\n",
      "Epoch 2/200\n",
      "1513/1513 [==============================] - 0s 40us/step - loss: 0.2678 - acc: 0.9293\n",
      "Epoch 3/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2607 - acc: 0.9326\n",
      "Epoch 4/200\n",
      "1513/1513 [==============================] - 0s 44us/step - loss: 0.2660 - acc: 0.9266\n",
      "Epoch 5/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2631 - acc: 0.9339\n",
      "Epoch 6/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2626 - acc: 0.9352\n",
      "Epoch 7/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2661 - acc: 0.9306\n",
      "Epoch 8/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2619 - acc: 0.9319\n",
      "Epoch 9/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2612 - acc: 0.9319\n",
      "Epoch 10/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2680 - acc: 0.9286\n",
      "Epoch 11/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2639 - acc: 0.9332\n",
      "Epoch 12/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2611 - acc: 0.9306\n",
      "Epoch 13/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2637 - acc: 0.9313\n",
      "Epoch 14/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2566 - acc: 0.9346\n",
      "Epoch 15/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2698 - acc: 0.9293\n",
      "Epoch 16/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2665 - acc: 0.9286\n",
      "Epoch 17/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2629 - acc: 0.9319\n",
      "Epoch 18/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2652 - acc: 0.9319\n",
      "Epoch 19/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2733 - acc: 0.9286\n",
      "Epoch 20/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2583 - acc: 0.9306\n",
      "Epoch 21/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2581 - acc: 0.9313\n",
      "Epoch 22/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2587 - acc: 0.9319\n",
      "Epoch 23/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.2633 - acc: 0.9352\n",
      "Epoch 24/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2595 - acc: 0.9280\n",
      "Epoch 25/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2660 - acc: 0.9293\n",
      "Epoch 26/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2535 - acc: 0.9332\n",
      "Epoch 27/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2672 - acc: 0.9313\n",
      "Epoch 28/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2620 - acc: 0.9280\n",
      "Epoch 29/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2627 - acc: 0.9286\n",
      "Epoch 30/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2626 - acc: 0.9253\n",
      "Epoch 31/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2517 - acc: 0.9392\n",
      "Epoch 32/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2636 - acc: 0.9293\n",
      "Epoch 33/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2643 - acc: 0.9326\n",
      "Epoch 34/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2572 - acc: 0.9359\n",
      "Epoch 35/200\n",
      "1513/1513 [==============================] - 0s 67us/step - loss: 0.2629 - acc: 0.9313\n",
      "Epoch 36/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2646 - acc: 0.9326\n",
      "Epoch 37/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2578 - acc: 0.9326\n",
      "Epoch 38/200\n",
      "1513/1513 [==============================] - 0s 80us/step - loss: 0.2672 - acc: 0.9332\n",
      "Epoch 39/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2697 - acc: 0.9273\n",
      "Epoch 40/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2570 - acc: 0.9326\n",
      "Epoch 41/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2650 - acc: 0.9319\n",
      "Epoch 42/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2542 - acc: 0.9346\n",
      "Epoch 43/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2597 - acc: 0.9372\n",
      "Epoch 44/200\n",
      "1513/1513 [==============================] - 0s 73us/step - loss: 0.2577 - acc: 0.9332\n",
      "Epoch 45/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2620 - acc: 0.9352\n",
      "Epoch 46/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2591 - acc: 0.9332\n",
      "Epoch 47/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2615 - acc: 0.9332\n",
      "Epoch 48/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2576 - acc: 0.9352\n",
      "Epoch 49/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2546 - acc: 0.9352\n",
      "Epoch 50/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2588 - acc: 0.9332\n",
      "Epoch 51/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2569 - acc: 0.9319\n",
      "Epoch 52/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2550 - acc: 0.9332\n",
      "Epoch 53/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2508 - acc: 0.9326\n",
      "Epoch 54/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2637 - acc: 0.9332\n",
      "Epoch 55/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2611 - acc: 0.9319\n",
      "Epoch 56/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2522 - acc: 0.9339\n",
      "Epoch 57/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2597 - acc: 0.9346\n",
      "Epoch 58/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2490 - acc: 0.9372\n",
      "Epoch 59/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2627 - acc: 0.9319\n",
      "Epoch 60/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2545 - acc: 0.9352\n",
      "Epoch 61/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2605 - acc: 0.9299\n",
      "Epoch 62/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2557 - acc: 0.9313\n",
      "Epoch 63/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2489 - acc: 0.9372\n",
      "Epoch 64/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2535 - acc: 0.9313\n",
      "Epoch 65/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2481 - acc: 0.9392: 0s - loss: 0.2574 - acc: 0.938\n",
      "Epoch 66/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2525 - acc: 0.9352\n",
      "Epoch 67/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2453 - acc: 0.9352\n",
      "Epoch 68/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2546 - acc: 0.9352\n",
      "Epoch 69/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2548 - acc: 0.9385\n",
      "Epoch 70/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2498 - acc: 0.9326\n",
      "Epoch 71/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2569 - acc: 0.9379\n",
      "Epoch 72/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2524 - acc: 0.9359\n",
      "Epoch 73/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2595 - acc: 0.9306\n",
      "Epoch 74/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2523 - acc: 0.9352\n",
      "Epoch 75/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2660 - acc: 0.9306\n",
      "Epoch 76/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2552 - acc: 0.9313\n",
      "Epoch 77/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2529 - acc: 0.9365\n",
      "Epoch 78/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2595 - acc: 0.9313\n",
      "Epoch 79/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2480 - acc: 0.9306\n",
      "Epoch 80/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2579 - acc: 0.9332\n",
      "Epoch 81/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2600 - acc: 0.9306\n",
      "Epoch 82/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2538 - acc: 0.9372\n",
      "Epoch 83/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2561 - acc: 0.9313\n",
      "Epoch 84/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2545 - acc: 0.9339\n",
      "Epoch 85/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2537 - acc: 0.9339\n",
      "Epoch 86/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2509 - acc: 0.9365\n",
      "Epoch 87/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2548 - acc: 0.9326\n",
      "Epoch 88/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2516 - acc: 0.9286\n",
      "Epoch 89/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2518 - acc: 0.9326\n",
      "Epoch 90/200\n",
      "1513/1513 [==============================] - 0s 62us/step - loss: 0.2496 - acc: 0.9319\n",
      "Epoch 91/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2552 - acc: 0.9332\n",
      "Epoch 92/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2512 - acc: 0.9346\n",
      "Epoch 93/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2521 - acc: 0.9372\n",
      "Epoch 94/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2465 - acc: 0.9399\n",
      "Epoch 95/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2541 - acc: 0.9346\n",
      "Epoch 96/200\n",
      "1513/1513 [==============================] - 0s 81us/step - loss: 0.2476 - acc: 0.9352\n",
      "Epoch 97/200\n",
      "1513/1513 [==============================] - 0s 76us/step - loss: 0.2510 - acc: 0.9346\n",
      "Epoch 98/200\n",
      "1513/1513 [==============================] - 0s 93us/step - loss: 0.2496 - acc: 0.9346\n",
      "Epoch 99/200\n",
      "1513/1513 [==============================] - 0s 66us/step - loss: 0.2481 - acc: 0.9326\n",
      "Epoch 100/200\n",
      "1513/1513 [==============================] - 0s 58us/step - loss: 0.2572 - acc: 0.9352\n",
      "Epoch 101/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2511 - acc: 0.9339\n",
      "Epoch 102/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2495 - acc: 0.9399\n",
      "Epoch 103/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2501 - acc: 0.9332\n",
      "Epoch 104/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2488 - acc: 0.9379\n",
      "Epoch 105/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2477 - acc: 0.9372\n",
      "Epoch 106/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2519 - acc: 0.9392\n",
      "Epoch 107/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2512 - acc: 0.9339\n",
      "Epoch 108/200\n",
      "1513/1513 [==============================] - 0s 69us/step - loss: 0.2440 - acc: 0.9418\n",
      "Epoch 109/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2505 - acc: 0.9365\n",
      "Epoch 110/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2498 - acc: 0.9365\n",
      "Epoch 111/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2530 - acc: 0.9313\n",
      "Epoch 112/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2487 - acc: 0.9359\n",
      "Epoch 113/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2418 - acc: 0.9346\n",
      "Epoch 114/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2490 - acc: 0.9352\n",
      "Epoch 115/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2436 - acc: 0.9365\n",
      "Epoch 116/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2521 - acc: 0.9365\n",
      "Epoch 117/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2480 - acc: 0.9339\n",
      "Epoch 118/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2515 - acc: 0.9352\n",
      "Epoch 119/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2485 - acc: 0.9359\n",
      "Epoch 120/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2508 - acc: 0.9359\n",
      "Epoch 121/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2424 - acc: 0.9412\n",
      "Epoch 122/200\n",
      "1513/1513 [==============================] - 0s 65us/step - loss: 0.2487 - acc: 0.9372\n",
      "Epoch 123/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2491 - acc: 0.9365\n",
      "Epoch 124/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2514 - acc: 0.9332\n",
      "Epoch 125/200\n",
      "1513/1513 [==============================] - 0s 63us/step - loss: 0.2444 - acc: 0.9332: 0s - loss: 0.1990 - acc: 0.946\n",
      "Epoch 126/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2471 - acc: 0.9346\n",
      "Epoch 127/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2473 - acc: 0.9372\n",
      "Epoch 128/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2556 - acc: 0.9385\n",
      "Epoch 129/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2507 - acc: 0.9359\n",
      "Epoch 130/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2504 - acc: 0.9372\n",
      "Epoch 131/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2484 - acc: 0.9332\n",
      "Epoch 132/200\n",
      "1513/1513 [==============================] - 0s 61us/step - loss: 0.2487 - acc: 0.9365\n",
      "Epoch 133/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2486 - acc: 0.9379\n",
      "Epoch 134/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2415 - acc: 0.9346\n",
      "Epoch 135/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2452 - acc: 0.9379\n",
      "Epoch 136/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2462 - acc: 0.9365\n",
      "Epoch 137/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2462 - acc: 0.9379\n",
      "Epoch 138/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2466 - acc: 0.9365\n",
      "Epoch 139/200\n",
      "1513/1513 [==============================] - 0s 60us/step - loss: 0.2489 - acc: 0.9306\n",
      "Epoch 140/200\n",
      "1513/1513 [==============================] - 0s 59us/step - loss: 0.2458 - acc: 0.9359\n",
      "Epoch 141/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2434 - acc: 0.9372\n",
      "Epoch 142/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2429 - acc: 0.9385\n",
      "Epoch 143/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2499 - acc: 0.9385\n",
      "Epoch 144/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2439 - acc: 0.9359\n",
      "Epoch 145/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2449 - acc: 0.9365\n",
      "Epoch 146/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2374 - acc: 0.9392\n",
      "Epoch 147/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2415 - acc: 0.9359\n",
      "Epoch 148/200\n",
      "1513/1513 [==============================] - ETA: 0s - loss: 0.2380 - acc: 0.932 - 0s 57us/step - loss: 0.2489 - acc: 0.9372\n",
      "Epoch 149/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2462 - acc: 0.9352\n",
      "Epoch 150/200\n",
      "1513/1513 [==============================] - 0s 64us/step - loss: 0.2521 - acc: 0.9405\n",
      "Epoch 151/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2512 - acc: 0.9359\n",
      "Epoch 152/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2429 - acc: 0.9412: 0s - loss: 0.2360 - acc: 0.939\n",
      "Epoch 153/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2492 - acc: 0.9332\n",
      "Epoch 154/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2435 - acc: 0.9326\n",
      "Epoch 155/200\n",
      "1513/1513 [==============================] - 0s 56us/step - loss: 0.2467 - acc: 0.9399\n",
      "Epoch 156/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2455 - acc: 0.9372\n",
      "Epoch 157/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2452 - acc: 0.9379\n",
      "Epoch 158/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2420 - acc: 0.9365\n",
      "Epoch 159/200\n",
      "1513/1513 [==============================] - 0s 54us/step - loss: 0.2387 - acc: 0.9372\n",
      "Epoch 160/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2492 - acc: 0.9359\n",
      "Epoch 161/200\n",
      "1513/1513 [==============================] - 0s 55us/step - loss: 0.2440 - acc: 0.9359\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2380 - acc: 0.9399\n",
      "Epoch 163/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2448 - acc: 0.9372\n",
      "Epoch 164/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2423 - acc: 0.9372\n",
      "Epoch 165/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2443 - acc: 0.9392\n",
      "Epoch 166/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2365 - acc: 0.9385\n",
      "Epoch 167/200\n",
      "1513/1513 [==============================] - 0s 45us/step - loss: 0.2432 - acc: 0.9405\n",
      "Epoch 168/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2386 - acc: 0.9399\n",
      "Epoch 169/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2455 - acc: 0.9326\n",
      "Epoch 170/200\n",
      "1513/1513 [==============================] - 0s 45us/step - loss: 0.2432 - acc: 0.9372\n",
      "Epoch 171/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2456 - acc: 0.9372\n",
      "Epoch 172/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2408 - acc: 0.9379\n",
      "Epoch 173/200\n",
      "1513/1513 [==============================] - 0s 43us/step - loss: 0.2430 - acc: 0.9379\n",
      "Epoch 174/200\n",
      "1513/1513 [==============================] - 0s 50us/step - loss: 0.2366 - acc: 0.9346\n",
      "Epoch 175/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2527 - acc: 0.9372\n",
      "Epoch 176/200\n",
      "1513/1513 [==============================] - 0s 42us/step - loss: 0.2395 - acc: 0.9365\n",
      "Epoch 177/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2374 - acc: 0.9359\n",
      "Epoch 178/200\n",
      "1513/1513 [==============================] - 0s 57us/step - loss: 0.2441 - acc: 0.9372\n",
      "Epoch 179/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2422 - acc: 0.9425\n",
      "Epoch 180/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2454 - acc: 0.9365\n",
      "Epoch 181/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2404 - acc: 0.9379\n",
      "Epoch 182/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2403 - acc: 0.9346\n",
      "Epoch 183/200\n",
      "1513/1513 [==============================] - 0s 45us/step - loss: 0.2410 - acc: 0.9372\n",
      "Epoch 184/200\n",
      "1513/1513 [==============================] - 0s 42us/step - loss: 0.2452 - acc: 0.9379\n",
      "Epoch 185/200\n",
      "1513/1513 [==============================] - 0s 52us/step - loss: 0.2411 - acc: 0.9392\n",
      "Epoch 186/200\n",
      "1513/1513 [==============================] - 0s 46us/step - loss: 0.2411 - acc: 0.9372\n",
      "Epoch 187/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2363 - acc: 0.9392\n",
      "Epoch 188/200\n",
      "1513/1513 [==============================] - 0s 44us/step - loss: 0.2363 - acc: 0.9412\n",
      "Epoch 189/200\n",
      "1513/1513 [==============================] - 0s 49us/step - loss: 0.2412 - acc: 0.9372\n",
      "Epoch 190/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2476 - acc: 0.9372\n",
      "Epoch 191/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2389 - acc: 0.9365\n",
      "Epoch 192/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2421 - acc: 0.9379\n",
      "Epoch 193/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2449 - acc: 0.9346\n",
      "Epoch 194/200\n",
      "1513/1513 [==============================] - 0s 53us/step - loss: 0.2351 - acc: 0.9432\n",
      "Epoch 195/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2374 - acc: 0.9359\n",
      "Epoch 196/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2439 - acc: 0.9346\n",
      "Epoch 197/200\n",
      "1513/1513 [==============================] - 0s 51us/step - loss: 0.2404 - acc: 0.9372\n",
      "Epoch 198/200\n",
      "1513/1513 [==============================] - ETA: 0s - loss: 0.2362 - acc: 0.941 - 0s 53us/step - loss: 0.2366 - acc: 0.9392\n",
      "Epoch 199/200\n",
      "1513/1513 [==============================] - 0s 48us/step - loss: 0.2367 - acc: 0.9392\n",
      "Epoch 200/200\n",
      "1513/1513 [==============================] - 0s 47us/step - loss: 0.2442 - acc: 0.9372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b92ee5f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import read2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = read2.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'read2' from 'C:\\\\Users\\\\Jay\\\\Desktop\\\\capstone\\\\read2.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(read2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.create_model(shape=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 497 samples\n",
      "Epoch 1/200\n",
      "1006/1006 [==============================] - 0s 268us/step - loss: 3.5264 - acc: 0.5656 - val_loss: 2.5726 - val_acc: 0.6378\n",
      "Epoch 2/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 2.0496 - acc: 0.6431 - val_loss: 1.8263 - val_acc: 0.6640\n",
      "Epoch 3/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 1.6779 - acc: 0.6799 - val_loss: 1.5373 - val_acc: 0.7022\n",
      "Epoch 4/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 1.4258 - acc: 0.7068 - val_loss: 1.3381 - val_acc: 0.6982\n",
      "Epoch 5/200\n",
      "1006/1006 [==============================] - 0s 116us/step - loss: 1.2823 - acc: 0.7217 - val_loss: 1.2937 - val_acc: 0.7183\n",
      "Epoch 6/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 1.2243 - acc: 0.7316 - val_loss: 1.1775 - val_acc: 0.7465\n",
      "Epoch 7/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 1.1586 - acc: 0.7485 - val_loss: 1.1914 - val_acc: 0.7264\n",
      "Epoch 8/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 1.0748 - acc: 0.7584 - val_loss: 1.0817 - val_acc: 0.7606\n",
      "Epoch 9/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 1.0031 - acc: 0.7744 - val_loss: 1.0487 - val_acc: 0.7626\n",
      "Epoch 10/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.9560 - acc: 0.7694 - val_loss: 1.0170 - val_acc: 0.7746\n",
      "Epoch 11/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.8930 - acc: 0.7803 - val_loss: 0.9791 - val_acc: 0.7726\n",
      "Epoch 12/200\n",
      "1006/1006 [==============================] - 0s 144us/step - loss: 0.8535 - acc: 0.7913 - val_loss: 0.9221 - val_acc: 0.7847\n",
      "Epoch 13/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.8126 - acc: 0.7952 - val_loss: 0.9462 - val_acc: 0.7726\n",
      "Epoch 14/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.7830 - acc: 0.8032 - val_loss: 0.8854 - val_acc: 0.7907\n",
      "Epoch 15/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.7553 - acc: 0.8022 - val_loss: 1.0537 - val_acc: 0.7465\n",
      "Epoch 16/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.7496 - acc: 0.8191 - val_loss: 0.8795 - val_acc: 0.7948\n",
      "Epoch 17/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 0.7399 - acc: 0.8121 - val_loss: 0.8441 - val_acc: 0.8008\n",
      "Epoch 18/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.6968 - acc: 0.8171 - val_loss: 0.9149 - val_acc: 0.7767\n",
      "Epoch 19/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.7058 - acc: 0.8260 - val_loss: 0.8808 - val_acc: 0.7827\n",
      "Epoch 20/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.6856 - acc: 0.8250 - val_loss: 0.8823 - val_acc: 0.7746\n",
      "Epoch 21/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.6717 - acc: 0.8290 - val_loss: 0.8444 - val_acc: 0.7948\n",
      "Epoch 22/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.6641 - acc: 0.8290 - val_loss: 0.8350 - val_acc: 0.7907\n",
      "Epoch 23/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.6460 - acc: 0.8320 - val_loss: 0.9195 - val_acc: 0.7485\n",
      "Epoch 24/200\n",
      "1006/1006 [==============================] - 0s 144us/step - loss: 0.6384 - acc: 0.8350 - val_loss: 0.8325 - val_acc: 0.7867\n",
      "Epoch 25/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.6330 - acc: 0.8320 - val_loss: 0.8877 - val_acc: 0.7746\n",
      "Epoch 26/200\n",
      "1006/1006 [==============================] - 0s 144us/step - loss: 0.6231 - acc: 0.8370 - val_loss: 0.8914 - val_acc: 0.7646\n",
      "Epoch 27/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.6162 - acc: 0.8380 - val_loss: 0.8491 - val_acc: 0.7686\n",
      "Epoch 28/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.6121 - acc: 0.8360 - val_loss: 0.8464 - val_acc: 0.7968\n",
      "Epoch 29/200\n",
      "1006/1006 [==============================] - 0s 137us/step - loss: 0.6029 - acc: 0.8300 - val_loss: 0.8823 - val_acc: 0.7827\n",
      "Epoch 30/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.5950 - acc: 0.8469 - val_loss: 0.8655 - val_acc: 0.7928\n",
      "Epoch 31/200\n",
      "1006/1006 [==============================] - 0s 165us/step - loss: 0.5946 - acc: 0.8439 - val_loss: 0.8416 - val_acc: 0.7847\n",
      "Epoch 32/200\n",
      "1006/1006 [==============================] - 0s 168us/step - loss: 0.5786 - acc: 0.8529 - val_loss: 0.8402 - val_acc: 0.7988\n",
      "Epoch 33/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.5863 - acc: 0.8529 - val_loss: 0.8668 - val_acc: 0.7847\n",
      "Epoch 34/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.5640 - acc: 0.8489 - val_loss: 0.8520 - val_acc: 0.7988\n",
      "Epoch 35/200\n",
      "1006/1006 [==============================] - 0s 162us/step - loss: 0.5678 - acc: 0.8549 - val_loss: 0.8584 - val_acc: 0.7907\n",
      "Epoch 36/200\n",
      "1006/1006 [==============================] - 0s 167us/step - loss: 0.5533 - acc: 0.8499 - val_loss: 0.8406 - val_acc: 0.7988\n",
      "Epoch 37/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.5637 - acc: 0.8598 - val_loss: 0.8880 - val_acc: 0.7988\n",
      "Epoch 38/200\n",
      "1006/1006 [==============================] - 0s 141us/step - loss: 0.5590 - acc: 0.8539 - val_loss: 0.8682 - val_acc: 0.7867\n",
      "Epoch 39/200\n",
      "1006/1006 [==============================] - 0s 176us/step - loss: 0.5469 - acc: 0.8509 - val_loss: 0.8508 - val_acc: 0.8008\n",
      "Epoch 40/200\n",
      "1006/1006 [==============================] - 0s 159us/step - loss: 0.5457 - acc: 0.8519 - val_loss: 0.8376 - val_acc: 0.7948\n",
      "Epoch 41/200\n",
      "1006/1006 [==============================] - 0s 137us/step - loss: 0.5270 - acc: 0.8608 - val_loss: 0.8322 - val_acc: 0.8129\n",
      "Epoch 42/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.5251 - acc: 0.8588 - val_loss: 0.8371 - val_acc: 0.7988\n",
      "Epoch 43/200\n",
      "1006/1006 [==============================] - 0s 173us/step - loss: 0.5266 - acc: 0.8588 - val_loss: 0.8674 - val_acc: 0.7948\n",
      "Epoch 44/200\n",
      "1006/1006 [==============================] - 0s 175us/step - loss: 0.5166 - acc: 0.8598 - val_loss: 0.8318 - val_acc: 0.7968\n",
      "Epoch 45/200\n",
      "1006/1006 [==============================] - 0s 137us/step - loss: 0.5241 - acc: 0.8569 - val_loss: 0.8177 - val_acc: 0.8048\n",
      "Epoch 46/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.5044 - acc: 0.8618 - val_loss: 0.8834 - val_acc: 0.7968\n",
      "Epoch 47/200\n",
      "1006/1006 [==============================] - 0s 161us/step - loss: 0.5172 - acc: 0.8648 - val_loss: 0.8676 - val_acc: 0.7948\n",
      "Epoch 48/200\n",
      "1006/1006 [==============================] - 0s 157us/step - loss: 0.5000 - acc: 0.8638 - val_loss: 0.9037 - val_acc: 0.7948\n",
      "Epoch 49/200\n",
      "1006/1006 [==============================] - 0s 156us/step - loss: 0.5031 - acc: 0.8648 - val_loss: 0.8233 - val_acc: 0.8149\n",
      "Epoch 50/200\n",
      "1006/1006 [==============================] - 0s 159us/step - loss: 0.5009 - acc: 0.8678 - val_loss: 0.8383 - val_acc: 0.8149\n",
      "Epoch 51/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.5018 - acc: 0.8648 - val_loss: 0.8563 - val_acc: 0.8109\n",
      "Epoch 52/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.4904 - acc: 0.8668 - val_loss: 0.8282 - val_acc: 0.8149\n",
      "Epoch 53/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.4872 - acc: 0.8718 - val_loss: 0.8658 - val_acc: 0.8089\n",
      "Epoch 54/200\n",
      "1006/1006 [==============================] - 0s 149us/step - loss: 0.4975 - acc: 0.8668 - val_loss: 0.8550 - val_acc: 0.8048\n",
      "Epoch 55/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.4742 - acc: 0.8777 - val_loss: 0.8327 - val_acc: 0.8109\n",
      "Epoch 56/200\n",
      "1006/1006 [==============================] - 0s 144us/step - loss: 0.4764 - acc: 0.8807 - val_loss: 0.8694 - val_acc: 0.8089\n",
      "Epoch 57/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.4781 - acc: 0.8698 - val_loss: 0.8817 - val_acc: 0.8109\n",
      "Epoch 58/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.4818 - acc: 0.8738 - val_loss: 0.8530 - val_acc: 0.8149\n",
      "Epoch 59/200\n",
      "1006/1006 [==============================] - 0s 150us/step - loss: 0.4800 - acc: 0.8748 - val_loss: 0.8529 - val_acc: 0.8068\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 0s 155us/step - loss: 0.4803 - acc: 0.8728 - val_loss: 0.8549 - val_acc: 0.8048\n",
      "Epoch 61/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.4774 - acc: 0.8678 - val_loss: 0.8375 - val_acc: 0.8109\n",
      "Epoch 62/200\n",
      "1006/1006 [==============================] - 0s 105us/step - loss: 0.4644 - acc: 0.8777 - val_loss: 0.8874 - val_acc: 0.8169\n",
      "Epoch 63/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.4865 - acc: 0.8708 - val_loss: 0.8460 - val_acc: 0.8109\n",
      "Epoch 64/200\n",
      "1006/1006 [==============================] - 0s 119us/step - loss: 0.4625 - acc: 0.8797 - val_loss: 0.8427 - val_acc: 0.8189\n",
      "Epoch 65/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.4711 - acc: 0.8757 - val_loss: 0.8906 - val_acc: 0.8068\n",
      "Epoch 66/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.4600 - acc: 0.8767 - val_loss: 0.9112 - val_acc: 0.8129\n",
      "Epoch 67/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 0.4667 - acc: 0.8698 - val_loss: 0.8503 - val_acc: 0.8169\n",
      "Epoch 68/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.4619 - acc: 0.8718 - val_loss: 0.8384 - val_acc: 0.8310\n",
      "Epoch 69/200\n",
      "1006/1006 [==============================] - 0s 122us/step - loss: 0.4637 - acc: 0.8757 - val_loss: 0.8401 - val_acc: 0.8249\n",
      "Epoch 70/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.4575 - acc: 0.8797 - val_loss: 0.8714 - val_acc: 0.8229\n",
      "Epoch 71/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.4531 - acc: 0.8827 - val_loss: 0.8899 - val_acc: 0.8129\n",
      "Epoch 72/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.4528 - acc: 0.8767 - val_loss: 0.8484 - val_acc: 0.8209\n",
      "Epoch 73/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.4638 - acc: 0.8767 - val_loss: 0.8518 - val_acc: 0.8290\n",
      "Epoch 74/200\n",
      "1006/1006 [==============================] - 0s 124us/step - loss: 0.4471 - acc: 0.8847 - val_loss: 0.9011 - val_acc: 0.8129\n",
      "Epoch 75/200\n",
      "1006/1006 [==============================] - 0s 153us/step - loss: 0.4510 - acc: 0.8777 - val_loss: 0.8664 - val_acc: 0.8189\n",
      "Epoch 76/200\n",
      "1006/1006 [==============================] - 0s 166us/step - loss: 0.4646 - acc: 0.8748 - val_loss: 0.8916 - val_acc: 0.8189\n",
      "Epoch 77/200\n",
      "1006/1006 [==============================] - 0s 142us/step - loss: 0.4415 - acc: 0.8887 - val_loss: 0.9022 - val_acc: 0.8149\n",
      "Epoch 78/200\n",
      "1006/1006 [==============================] - 0s 152us/step - loss: 0.4494 - acc: 0.8857 - val_loss: 0.8639 - val_acc: 0.8270\n",
      "Epoch 79/200\n",
      "1006/1006 [==============================] - 0s 177us/step - loss: 0.4369 - acc: 0.8787 - val_loss: 0.8424 - val_acc: 0.8270\n",
      "Epoch 80/200\n",
      "1006/1006 [==============================] - 0s 196us/step - loss: 0.4523 - acc: 0.8857 - val_loss: 0.8672 - val_acc: 0.8229\n",
      "Epoch 81/200\n",
      "1006/1006 [==============================] - 0s 181us/step - loss: 0.4440 - acc: 0.8817 - val_loss: 0.9056 - val_acc: 0.8068\n",
      "Epoch 82/200\n",
      "1006/1006 [==============================] - 0s 187us/step - loss: 0.4309 - acc: 0.8867 - val_loss: 0.9059 - val_acc: 0.8089\n",
      "Epoch 83/200\n",
      "1006/1006 [==============================] - 0s 193us/step - loss: 0.4449 - acc: 0.8847 - val_loss: 0.8810 - val_acc: 0.8249\n",
      "Epoch 84/200\n",
      "1006/1006 [==============================] - 0s 164us/step - loss: 0.4329 - acc: 0.8857 - val_loss: 0.9378 - val_acc: 0.8089\n",
      "Epoch 85/200\n",
      "1006/1006 [==============================] - 0s 170us/step - loss: 0.4376 - acc: 0.8897 - val_loss: 0.8967 - val_acc: 0.8149\n",
      "Epoch 86/200\n",
      "1006/1006 [==============================] - 0s 186us/step - loss: 0.4349 - acc: 0.8887 - val_loss: 0.8872 - val_acc: 0.8149\n",
      "Epoch 87/200\n",
      "1006/1006 [==============================] - 0s 151us/step - loss: 0.4372 - acc: 0.8837 - val_loss: 0.8875 - val_acc: 0.8209\n",
      "Epoch 88/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.4330 - acc: 0.8946 - val_loss: 0.8665 - val_acc: 0.8290\n",
      "Epoch 89/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.4253 - acc: 0.8907 - val_loss: 0.8948 - val_acc: 0.8290\n",
      "Epoch 90/200\n",
      "1006/1006 [==============================] - 0s 146us/step - loss: 0.4332 - acc: 0.8897 - val_loss: 0.8988 - val_acc: 0.8249\n",
      "Epoch 91/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.4325 - acc: 0.8867 - val_loss: 0.8995 - val_acc: 0.8169\n",
      "Epoch 92/200\n",
      "1006/1006 [==============================] - 0s 202us/step - loss: 0.4331 - acc: 0.8867 - val_loss: 0.8955 - val_acc: 0.8270\n",
      "Epoch 93/200\n",
      "1006/1006 [==============================] - 0s 195us/step - loss: 0.4225 - acc: 0.8946 - val_loss: 0.9114 - val_acc: 0.8149\n",
      "Epoch 94/200\n",
      "1006/1006 [==============================] - 0s 185us/step - loss: 0.4196 - acc: 0.8897 - val_loss: 0.8983 - val_acc: 0.8229\n",
      "Epoch 95/200\n",
      "1006/1006 [==============================] - 0s 188us/step - loss: 0.4287 - acc: 0.8847 - val_loss: 0.9232 - val_acc: 0.8249\n",
      "Epoch 96/200\n",
      "1006/1006 [==============================] - 0s 160us/step - loss: 0.4325 - acc: 0.8907 - val_loss: 0.9054 - val_acc: 0.8249\n",
      "Epoch 97/200\n",
      "1006/1006 [==============================] - 0s 137us/step - loss: 0.4156 - acc: 0.8946 - val_loss: 0.9149 - val_acc: 0.8189\n",
      "Epoch 98/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.4286 - acc: 0.8936 - val_loss: 0.9268 - val_acc: 0.8189\n",
      "Epoch 99/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.4169 - acc: 0.8966 - val_loss: 0.9226 - val_acc: 0.8169\n",
      "Epoch 100/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.4073 - acc: 0.8867 - val_loss: 0.9112 - val_acc: 0.8209\n",
      "Epoch 101/200\n",
      "1006/1006 [==============================] - 0s 117us/step - loss: 0.4029 - acc: 0.8966 - val_loss: 0.9174 - val_acc: 0.8189\n",
      "Epoch 102/200\n",
      "1006/1006 [==============================] - 0s 145us/step - loss: 0.3929 - acc: 0.8917 - val_loss: 0.9008 - val_acc: 0.8229\n",
      "Epoch 103/200\n",
      "1006/1006 [==============================] - 0s 187us/step - loss: 0.3979 - acc: 0.8956 - val_loss: 0.9764 - val_acc: 0.8169\n",
      "Epoch 104/200\n",
      "1006/1006 [==============================] - 0s 191us/step - loss: 0.3939 - acc: 0.8966 - val_loss: 0.9010 - val_acc: 0.8270\n",
      "Epoch 105/200\n",
      "1006/1006 [==============================] - 0s 182us/step - loss: 0.4040 - acc: 0.8996 - val_loss: 0.9186 - val_acc: 0.8270\n",
      "Epoch 106/200\n",
      "1006/1006 [==============================] - 0s 199us/step - loss: 0.3880 - acc: 0.8996 - val_loss: 0.9272 - val_acc: 0.8209\n",
      "Epoch 107/200\n",
      "1006/1006 [==============================] - 0s 166us/step - loss: 0.3981 - acc: 0.8996 - val_loss: 0.9083 - val_acc: 0.8209\n",
      "Epoch 108/200\n",
      "1006/1006 [==============================] - 0s 175us/step - loss: 0.3924 - acc: 0.9006 - val_loss: 0.9269 - val_acc: 0.8209\n",
      "Epoch 109/200\n",
      "1006/1006 [==============================] - 0s 176us/step - loss: 0.4022 - acc: 0.8986 - val_loss: 0.9257 - val_acc: 0.8149\n",
      "Epoch 110/200\n",
      "1006/1006 [==============================] - 0s 180us/step - loss: 0.3854 - acc: 0.9016 - val_loss: 0.9295 - val_acc: 0.8189\n",
      "Epoch 111/200\n",
      "1006/1006 [==============================] - 0s 187us/step - loss: 0.3893 - acc: 0.9016 - val_loss: 0.9269 - val_acc: 0.8169\n",
      "Epoch 112/200\n",
      "1006/1006 [==============================] - 0s 171us/step - loss: 0.3937 - acc: 0.9036 - val_loss: 0.9339 - val_acc: 0.8169\n",
      "Epoch 113/200\n",
      "1006/1006 [==============================] - 0s 173us/step - loss: 0.3950 - acc: 0.8926 - val_loss: 0.9363 - val_acc: 0.8189\n",
      "Epoch 114/200\n",
      "1006/1006 [==============================] - 0s 185us/step - loss: 0.3818 - acc: 0.8936 - val_loss: 0.9536 - val_acc: 0.8169\n",
      "Epoch 115/200\n",
      "1006/1006 [==============================] - 0s 178us/step - loss: 0.3904 - acc: 0.9026 - val_loss: 0.9331 - val_acc: 0.8169\n",
      "Epoch 116/200\n",
      "1006/1006 [==============================] - 0s 178us/step - loss: 0.3843 - acc: 0.8996 - val_loss: 0.9167 - val_acc: 0.8129\n",
      "Epoch 117/200\n",
      "1006/1006 [==============================] - 0s 171us/step - loss: 0.3764 - acc: 0.8956 - val_loss: 0.9948 - val_acc: 0.8129\n",
      "Epoch 118/200\n",
      "1006/1006 [==============================] - 0s 170us/step - loss: 0.3881 - acc: 0.9006 - val_loss: 0.9792 - val_acc: 0.8109\n",
      "Epoch 119/200\n",
      "1006/1006 [==============================] - 0s 171us/step - loss: 0.3731 - acc: 0.9085 - val_loss: 0.9194 - val_acc: 0.8249\n",
      "Epoch 120/200\n",
      "1006/1006 [==============================] - 0s 168us/step - loss: 0.3867 - acc: 0.9026 - val_loss: 0.9714 - val_acc: 0.8169\n",
      "Epoch 121/200\n",
      "1006/1006 [==============================] - 0s 151us/step - loss: 0.3972 - acc: 0.8986 - val_loss: 0.9283 - val_acc: 0.8149\n",
      "Epoch 122/200\n",
      "1006/1006 [==============================] - 0s 173us/step - loss: 0.3776 - acc: 0.9076 - val_loss: 0.9460 - val_acc: 0.8169\n",
      "Epoch 123/200\n",
      "1006/1006 [==============================] - 0s 167us/step - loss: 0.3759 - acc: 0.9056 - val_loss: 0.9166 - val_acc: 0.8249\n",
      "Epoch 124/200\n",
      "1006/1006 [==============================] - 0s 139us/step - loss: 0.3882 - acc: 0.8956 - val_loss: 0.9804 - val_acc: 0.8089\n",
      "Epoch 125/200\n",
      "1006/1006 [==============================] - 0s 105us/step - loss: 0.3698 - acc: 0.9066 - val_loss: 0.9709 - val_acc: 0.8129\n",
      "Epoch 126/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.3753 - acc: 0.8996 - val_loss: 0.9880 - val_acc: 0.8229\n",
      "Epoch 127/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.3741 - acc: 0.9115 - val_loss: 0.9717 - val_acc: 0.8209\n",
      "Epoch 128/200\n",
      "1006/1006 [==============================] - 0s 142us/step - loss: 0.3697 - acc: 0.9046 - val_loss: 0.9574 - val_acc: 0.8189\n",
      "Epoch 129/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.3657 - acc: 0.9085 - val_loss: 0.9464 - val_acc: 0.8270\n",
      "Epoch 130/200\n",
      "1006/1006 [==============================] - 0s 121us/step - loss: 0.3731 - acc: 0.9036 - val_loss: 0.9833 - val_acc: 0.8209\n",
      "Epoch 131/200\n",
      "1006/1006 [==============================] - 0s 124us/step - loss: 0.3741 - acc: 0.9056 - val_loss: 0.9728 - val_acc: 0.8189\n",
      "Epoch 132/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.3646 - acc: 0.9115 - val_loss: 0.9795 - val_acc: 0.8129\n",
      "Epoch 133/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.3677 - acc: 0.9085 - val_loss: 0.9741 - val_acc: 0.8229\n",
      "Epoch 134/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 0.3681 - acc: 0.9115 - val_loss: 0.9680 - val_acc: 0.8149\n",
      "Epoch 135/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.3722 - acc: 0.8996 - val_loss: 0.9798 - val_acc: 0.8189\n",
      "Epoch 136/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.3788 - acc: 0.9076 - val_loss: 0.9679 - val_acc: 0.8109\n",
      "Epoch 137/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.3537 - acc: 0.9145 - val_loss: 0.9726 - val_acc: 0.8169\n",
      "Epoch 138/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.3611 - acc: 0.9056 - val_loss: 0.9674 - val_acc: 0.8149\n",
      "Epoch 139/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 0.3531 - acc: 0.9085 - val_loss: 1.0604 - val_acc: 0.8048\n",
      "Epoch 140/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.3790 - acc: 0.9056 - val_loss: 0.9939 - val_acc: 0.8270\n",
      "Epoch 141/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.3660 - acc: 0.9135 - val_loss: 0.9911 - val_acc: 0.8209\n",
      "Epoch 142/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.3606 - acc: 0.9076 - val_loss: 0.9872 - val_acc: 0.8229\n",
      "Epoch 143/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.3586 - acc: 0.9085 - val_loss: 0.9802 - val_acc: 0.8229\n",
      "Epoch 144/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.3419 - acc: 0.9135 - val_loss: 1.0012 - val_acc: 0.8149\n",
      "Epoch 145/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.3664 - acc: 0.9125 - val_loss: 1.0077 - val_acc: 0.8209\n",
      "Epoch 146/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.3509 - acc: 0.9145 - val_loss: 1.0293 - val_acc: 0.8129\n",
      "Epoch 147/200\n",
      "1006/1006 [==============================] - 0s 124us/step - loss: 0.3529 - acc: 0.9145 - val_loss: 1.0282 - val_acc: 0.8028\n",
      "Epoch 148/200\n",
      "1006/1006 [==============================] - 0s 121us/step - loss: 0.3652 - acc: 0.9135 - val_loss: 1.0110 - val_acc: 0.8109\n",
      "Epoch 149/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.3470 - acc: 0.9125 - val_loss: 1.0113 - val_acc: 0.8169\n",
      "Epoch 150/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.3448 - acc: 0.9125 - val_loss: 1.0116 - val_acc: 0.8209\n",
      "Epoch 151/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.3546 - acc: 0.9115 - val_loss: 1.0554 - val_acc: 0.8028\n",
      "Epoch 152/200\n",
      "1006/1006 [==============================] - 0s 139us/step - loss: 0.3581 - acc: 0.9076 - val_loss: 1.0202 - val_acc: 0.8028\n",
      "Epoch 153/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.3492 - acc: 0.9155 - val_loss: 1.0065 - val_acc: 0.8169\n",
      "Epoch 154/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.3353 - acc: 0.9185 - val_loss: 1.0108 - val_acc: 0.8209\n",
      "Epoch 155/200\n",
      "1006/1006 [==============================] - ETA: 0s - loss: 0.3595 - acc: 0.913 - 0s 112us/step - loss: 0.3539 - acc: 0.9125 - val_loss: 1.0091 - val_acc: 0.8209\n",
      "Epoch 156/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.3454 - acc: 0.9145 - val_loss: 1.0390 - val_acc: 0.8169\n",
      "Epoch 157/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.3585 - acc: 0.9105 - val_loss: 1.0132 - val_acc: 0.8149\n",
      "Epoch 158/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.3362 - acc: 0.9135 - val_loss: 1.0225 - val_acc: 0.8149\n",
      "Epoch 159/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.3387 - acc: 0.9135 - val_loss: 1.0489 - val_acc: 0.8109\n",
      "Epoch 160/200\n",
      "1006/1006 [==============================] - 0s 122us/step - loss: 0.3420 - acc: 0.9105 - val_loss: 1.0746 - val_acc: 0.8109\n",
      "Epoch 161/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 0.3350 - acc: 0.9155 - val_loss: 1.0311 - val_acc: 0.8109\n",
      "Epoch 162/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.3465 - acc: 0.9135 - val_loss: 1.0542 - val_acc: 0.8068\n",
      "Epoch 163/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.3574 - acc: 0.9085 - val_loss: 1.0646 - val_acc: 0.8028\n",
      "Epoch 164/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.3435 - acc: 0.9165 - val_loss: 1.0520 - val_acc: 0.8089\n",
      "Epoch 165/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.3425 - acc: 0.9145 - val_loss: 1.0308 - val_acc: 0.8109\n",
      "Epoch 166/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.3323 - acc: 0.9155 - val_loss: 1.1076 - val_acc: 0.8129\n",
      "Epoch 167/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.3323 - acc: 0.9195 - val_loss: 1.0491 - val_acc: 0.8149\n",
      "Epoch 168/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.3334 - acc: 0.9185 - val_loss: 1.0374 - val_acc: 0.8089\n",
      "Epoch 169/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.3430 - acc: 0.9135 - val_loss: 1.0329 - val_acc: 0.8109\n",
      "Epoch 170/200\n",
      "1006/1006 [==============================] - 0s 142us/step - loss: 0.3317 - acc: 0.9195 - val_loss: 1.0673 - val_acc: 0.8149\n",
      "Epoch 171/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.3267 - acc: 0.9135 - val_loss: 1.0451 - val_acc: 0.8129\n",
      "Epoch 172/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.3340 - acc: 0.9225 - val_loss: 1.0486 - val_acc: 0.8149\n",
      "Epoch 173/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.3331 - acc: 0.9215 - val_loss: 1.0536 - val_acc: 0.8149\n",
      "Epoch 174/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.3269 - acc: 0.9185 - val_loss: 1.0638 - val_acc: 0.8109\n",
      "Epoch 175/200\n",
      "1006/1006 [==============================] - 0s 174us/step - loss: 0.3301 - acc: 0.9185 - val_loss: 1.0992 - val_acc: 0.8129\n",
      "Epoch 176/200\n",
      "1006/1006 [==============================] - 0s 175us/step - loss: 0.3324 - acc: 0.9165 - val_loss: 1.0762 - val_acc: 0.8109\n",
      "Epoch 177/200\n",
      "1006/1006 [==============================] - 0s 168us/step - loss: 0.3262 - acc: 0.9205 - val_loss: 1.0762 - val_acc: 0.8149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "1006/1006 [==============================] - 0s 171us/step - loss: 0.3377 - acc: 0.9175 - val_loss: 1.0511 - val_acc: 0.8068\n",
      "Epoch 179/200\n",
      "1006/1006 [==============================] - 0s 160us/step - loss: 0.3293 - acc: 0.9165 - val_loss: 1.0670 - val_acc: 0.8068\n",
      "Epoch 180/200\n",
      "1006/1006 [==============================] - 0s 148us/step - loss: 0.3207 - acc: 0.9175 - val_loss: 1.0714 - val_acc: 0.8048\n",
      "Epoch 181/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.3266 - acc: 0.9235 - val_loss: 1.0833 - val_acc: 0.8109\n",
      "Epoch 182/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.3306 - acc: 0.9145 - val_loss: 1.0663 - val_acc: 0.8129\n",
      "Epoch 183/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 0.3275 - acc: 0.9264 - val_loss: 1.1202 - val_acc: 0.8089\n",
      "Epoch 184/200\n",
      "1006/1006 [==============================] - 0s 142us/step - loss: 0.3185 - acc: 0.9245 - val_loss: 1.0863 - val_acc: 0.8129\n",
      "Epoch 185/200\n",
      "1006/1006 [==============================] - 0s 167us/step - loss: 0.3273 - acc: 0.9195 - val_loss: 1.1191 - val_acc: 0.8028\n",
      "Epoch 186/200\n",
      "1006/1006 [==============================] - 0s 163us/step - loss: 0.3118 - acc: 0.9225 - val_loss: 1.1573 - val_acc: 0.8129\n",
      "Epoch 187/200\n",
      "1006/1006 [==============================] - 0s 170us/step - loss: 0.3226 - acc: 0.9195 - val_loss: 1.1019 - val_acc: 0.8089\n",
      "Epoch 188/200\n",
      "1006/1006 [==============================] - 0s 182us/step - loss: 0.3252 - acc: 0.9245 - val_loss: 1.0933 - val_acc: 0.7988\n",
      "Epoch 189/200\n",
      "1006/1006 [==============================] - 0s 175us/step - loss: 0.3263 - acc: 0.9245 - val_loss: 1.1324 - val_acc: 0.8008\n",
      "Epoch 190/200\n",
      "1006/1006 [==============================] - 0s 168us/step - loss: 0.3275 - acc: 0.9215 - val_loss: 1.0949 - val_acc: 0.8129\n",
      "Epoch 191/200\n",
      "1006/1006 [==============================] - 0s 147us/step - loss: 0.3174 - acc: 0.9235 - val_loss: 1.0980 - val_acc: 0.8068\n",
      "Epoch 192/200\n",
      "1006/1006 [==============================] - 0s 146us/step - loss: 0.3094 - acc: 0.9245 - val_loss: 1.1153 - val_acc: 0.8109\n",
      "Epoch 193/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.3188 - acc: 0.9225 - val_loss: 1.1209 - val_acc: 0.8149\n",
      "Epoch 194/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.3204 - acc: 0.9185 - val_loss: 1.1072 - val_acc: 0.8048\n",
      "Epoch 195/200\n",
      "1006/1006 [==============================] - 0s 146us/step - loss: 0.3166 - acc: 0.9245 - val_loss: 1.1112 - val_acc: 0.8169\n",
      "Epoch 196/200\n",
      "1006/1006 [==============================] - 0s 169us/step - loss: 0.3031 - acc: 0.9304 - val_loss: 1.1061 - val_acc: 0.8149\n",
      "Epoch 197/200\n",
      "1006/1006 [==============================] - 0s 183us/step - loss: 0.3195 - acc: 0.9225 - val_loss: 1.1387 - val_acc: 0.8109\n",
      "Epoch 198/200\n",
      "1006/1006 [==============================] - 0s 161us/step - loss: 0.3263 - acc: 0.9254 - val_loss: 1.1130 - val_acc: 0.8028\n",
      "Epoch 199/200\n",
      "1006/1006 [==============================] - 0s 171us/step - loss: 0.3230 - acc: 0.9175 - val_loss: 1.1108 - val_acc: 0.8149\n",
      "Epoch 200/200\n",
      "1006/1006 [==============================] - 0s 169us/step - loss: 0.3070 - acc: 0.9215 - val_loss: 1.1582 - val_acc: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3b2e02ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_label, epochs=200, validation_data=(test_data, test_label), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c8034d10446>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(read2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d8ee022b53ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'read2' is not defined"
     ]
    }
   ],
   "source": [
    "reload(read2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import read2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read2.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process.normalization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01098901 0.00544959 0.28139535 0.03761062 0.10344828 0.07142857\n",
      " 0.003125   0.00145985 0.00451128 0.         0.         0.\n",
      " 0.         0.15269461 0.28181818 0.11842105 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (27,) but got array with shape (18,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-06312cd507fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk_fold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\capstone\\k_fold.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(data, labels, epochs, folds)\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    134\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (27,) but got array with shape (18,)"
     ]
    }
   ],
   "source": [
    "k_fold.cross_validate(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 507 samples\n",
      "Epoch 1/200\n",
      "1006/1006 [==============================] - 0s 277us/step - loss: 2.1455 - acc: 0.3748 - val_loss: 1.9927 - val_acc: 0.3432\n",
      "Epoch 2/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 1.8639 - acc: 0.3579 - val_loss: 1.7619 - val_acc: 0.3728\n",
      "Epoch 3/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 1.6980 - acc: 0.5070 - val_loss: 1.6567 - val_acc: 0.5819\n",
      "Epoch 4/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 1.6158 - acc: 0.5447 - val_loss: 1.5914 - val_acc: 0.4990\n",
      "Epoch 5/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 1.5551 - acc: 0.4901 - val_loss: 1.5337 - val_acc: 0.5128\n",
      "Epoch 6/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 1.4995 - acc: 0.5000 - val_loss: 1.4815 - val_acc: 0.5306\n",
      "Epoch 7/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 1.4483 - acc: 0.5169 - val_loss: 1.4324 - val_acc: 0.5306\n",
      "Epoch 8/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 1.3994 - acc: 0.5189 - val_loss: 1.3876 - val_acc: 0.5523\n",
      "Epoch 9/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 1.3579 - acc: 0.5348 - val_loss: 1.3475 - val_acc: 0.5602\n",
      "Epoch 10/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 1.3189 - acc: 0.5497 - val_loss: 1.3141 - val_acc: 0.5759\n",
      "Epoch 11/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 1.2842 - acc: 0.5596 - val_loss: 1.2844 - val_acc: 0.6193\n",
      "Epoch 12/200\n",
      "1006/1006 [==============================] - 0s 118us/step - loss: 1.2527 - acc: 0.5875 - val_loss: 1.2513 - val_acc: 0.6016\n",
      "Epoch 13/200\n",
      "1006/1006 [==============================] - 0s 116us/step - loss: 1.2229 - acc: 0.5984 - val_loss: 1.2226 - val_acc: 0.6233\n",
      "Epoch 14/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 1.1950 - acc: 0.6183 - val_loss: 1.1962 - val_acc: 0.6331\n",
      "Epoch 15/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 1.1682 - acc: 0.6312 - val_loss: 1.1711 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 1.1440 - acc: 0.6650 - val_loss: 1.1484 - val_acc: 0.6529\n",
      "Epoch 17/200\n",
      "1006/1006 [==============================] - 0s 147us/step - loss: 1.1217 - acc: 0.6730 - val_loss: 1.1242 - val_acc: 0.6824\n",
      "Epoch 18/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 1.0972 - acc: 0.6849 - val_loss: 1.1024 - val_acc: 0.6824\n",
      "Epoch 19/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 1.0757 - acc: 0.6968 - val_loss: 1.0817 - val_acc: 0.7002\n",
      "Epoch 20/200\n",
      "1006/1006 [==============================] - 0s 162us/step - loss: 1.0546 - acc: 0.7018 - val_loss: 1.0639 - val_acc: 0.7337\n",
      "Epoch 21/200\n",
      "1006/1006 [==============================] - 0s 155us/step - loss: 1.0354 - acc: 0.7147 - val_loss: 1.0472 - val_acc: 0.7357\n",
      "Epoch 22/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 1.0180 - acc: 0.7266 - val_loss: 1.0280 - val_acc: 0.7278\n",
      "Epoch 23/200\n",
      "1006/1006 [==============================] - 0s 150us/step - loss: 0.9998 - acc: 0.7266 - val_loss: 1.0137 - val_acc: 0.7357\n",
      "Epoch 24/200\n",
      "1006/1006 [==============================] - 0s 149us/step - loss: 0.9837 - acc: 0.7336 - val_loss: 0.9998 - val_acc: 0.7377\n",
      "Epoch 25/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.9687 - acc: 0.7376 - val_loss: 0.9878 - val_acc: 0.7436\n",
      "Epoch 26/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.9543 - acc: 0.7455 - val_loss: 0.9757 - val_acc: 0.7377\n",
      "Epoch 27/200\n",
      "1006/1006 [==============================] - 0s 149us/step - loss: 0.9403 - acc: 0.7465 - val_loss: 0.9624 - val_acc: 0.7456\n",
      "Epoch 28/200\n",
      "1006/1006 [==============================] - 0s 159us/step - loss: 0.9272 - acc: 0.7445 - val_loss: 0.9515 - val_acc: 0.7475\n",
      "Epoch 29/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.9141 - acc: 0.7465 - val_loss: 0.9425 - val_acc: 0.7475\n",
      "Epoch 30/200\n",
      "1006/1006 [==============================] - 0s 137us/step - loss: 0.9024 - acc: 0.7465 - val_loss: 0.9320 - val_acc: 0.7416\n",
      "Epoch 31/200\n",
      "1006/1006 [==============================] - 0s 147us/step - loss: 0.8914 - acc: 0.7525 - val_loss: 0.9213 - val_acc: 0.7535\n",
      "Epoch 32/200\n",
      "1006/1006 [==============================] - 0s 155us/step - loss: 0.8793 - acc: 0.7545 - val_loss: 0.9144 - val_acc: 0.7475\n",
      "Epoch 33/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.8687 - acc: 0.7545 - val_loss: 0.9032 - val_acc: 0.7574\n",
      "Epoch 34/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.8584 - acc: 0.7584 - val_loss: 0.8959 - val_acc: 0.7594\n",
      "Epoch 35/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.8493 - acc: 0.7584 - val_loss: 0.8892 - val_acc: 0.7594\n",
      "Epoch 36/200\n",
      "1006/1006 [==============================] - 0s 157us/step - loss: 0.8394 - acc: 0.7584 - val_loss: 0.8825 - val_acc: 0.7416\n",
      "Epoch 37/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.8311 - acc: 0.7575 - val_loss: 0.8739 - val_acc: 0.7475\n",
      "Epoch 38/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 0.8238 - acc: 0.7575 - val_loss: 0.8654 - val_acc: 0.7535\n",
      "Epoch 39/200\n",
      "1006/1006 [==============================] - 0s 158us/step - loss: 0.8147 - acc: 0.7575 - val_loss: 0.8577 - val_acc: 0.7633\n",
      "Epoch 40/200\n",
      "1006/1006 [==============================] - 0s 150us/step - loss: 0.8063 - acc: 0.7584 - val_loss: 0.8522 - val_acc: 0.7613\n",
      "Epoch 41/200\n",
      "1006/1006 [==============================] - 0s 130us/step - loss: 0.7999 - acc: 0.7634 - val_loss: 0.8449 - val_acc: 0.7613\n",
      "Epoch 42/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.7918 - acc: 0.7614 - val_loss: 0.8381 - val_acc: 0.7594\n",
      "Epoch 43/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.7838 - acc: 0.7684 - val_loss: 0.8335 - val_acc: 0.7495\n",
      "Epoch 44/200\n",
      "1006/1006 [==============================] - 0s 139us/step - loss: 0.7781 - acc: 0.7634 - val_loss: 0.8282 - val_acc: 0.7475\n",
      "Epoch 45/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.7716 - acc: 0.7594 - val_loss: 0.8207 - val_acc: 0.7594\n",
      "Epoch 46/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.7660 - acc: 0.7664 - val_loss: 0.8159 - val_acc: 0.7633\n",
      "Epoch 47/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.7601 - acc: 0.7684 - val_loss: 0.8117 - val_acc: 0.7692\n",
      "Epoch 48/200\n",
      "1006/1006 [==============================] - 0s 148us/step - loss: 0.7529 - acc: 0.7694 - val_loss: 0.8070 - val_acc: 0.7751\n",
      "Epoch 49/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.7478 - acc: 0.7714 - val_loss: 0.8012 - val_acc: 0.7732\n",
      "Epoch 50/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 0.7435 - acc: 0.7684 - val_loss: 0.7980 - val_acc: 0.7653\n",
      "Epoch 51/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.7380 - acc: 0.7694 - val_loss: 0.7950 - val_acc: 0.7594\n",
      "Epoch 52/200\n",
      "1006/1006 [==============================] - 0s 132us/step - loss: 0.7329 - acc: 0.7744 - val_loss: 0.7898 - val_acc: 0.7712\n",
      "Epoch 53/200\n",
      "1006/1006 [==============================] - 0s 145us/step - loss: 0.7282 - acc: 0.7744 - val_loss: 0.7857 - val_acc: 0.7613\n",
      "Epoch 54/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.7239 - acc: 0.7714 - val_loss: 0.7816 - val_acc: 0.7673\n",
      "Epoch 55/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.7194 - acc: 0.7644 - val_loss: 0.7788 - val_acc: 0.7811\n",
      "Epoch 56/200\n",
      "1006/1006 [==============================] - 0s 151us/step - loss: 0.7156 - acc: 0.7734 - val_loss: 0.7752 - val_acc: 0.7712\n",
      "Epoch 57/200\n",
      "1006/1006 [==============================] - 0s 151us/step - loss: 0.7108 - acc: 0.7753 - val_loss: 0.7719 - val_acc: 0.7811\n",
      "Epoch 58/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.7072 - acc: 0.7813 - val_loss: 0.7705 - val_acc: 0.7554\n",
      "Epoch 59/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.7033 - acc: 0.7773 - val_loss: 0.7658 - val_acc: 0.7673\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.6995 - acc: 0.7793 - val_loss: 0.7634 - val_acc: 0.7791\n",
      "Epoch 61/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 0.6956 - acc: 0.7793 - val_loss: 0.7606 - val_acc: 0.7732\n",
      "Epoch 62/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.6928 - acc: 0.7763 - val_loss: 0.7591 - val_acc: 0.7732\n",
      "Epoch 63/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.6887 - acc: 0.7744 - val_loss: 0.7564 - val_acc: 0.7791\n",
      "Epoch 64/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.6853 - acc: 0.7763 - val_loss: 0.7563 - val_acc: 0.7811\n",
      "Epoch 65/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.6828 - acc: 0.7823 - val_loss: 0.7524 - val_acc: 0.7751\n",
      "Epoch 66/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.6800 - acc: 0.7813 - val_loss: 0.7510 - val_acc: 0.7890\n",
      "Epoch 67/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.6767 - acc: 0.7893 - val_loss: 0.7443 - val_acc: 0.7830\n",
      "Epoch 68/200\n",
      "1006/1006 [==============================] - 0s 122us/step - loss: 0.6734 - acc: 0.7803 - val_loss: 0.7462 - val_acc: 0.7988\n",
      "Epoch 69/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.6726 - acc: 0.7903 - val_loss: 0.7402 - val_acc: 0.7771\n",
      "Epoch 70/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.6687 - acc: 0.7813 - val_loss: 0.7389 - val_acc: 0.7751\n",
      "Epoch 71/200\n",
      "1006/1006 [==============================] - 0s 165us/step - loss: 0.6667 - acc: 0.7853 - val_loss: 0.7368 - val_acc: 0.7830\n",
      "Epoch 72/200\n",
      "1006/1006 [==============================] - 0s 200us/step - loss: 0.6643 - acc: 0.7783 - val_loss: 0.7363 - val_acc: 0.7949\n",
      "Epoch 73/200\n",
      "1006/1006 [==============================] - 0s 148us/step - loss: 0.6618 - acc: 0.7833 - val_loss: 0.7308 - val_acc: 0.7850\n",
      "Epoch 74/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.6601 - acc: 0.7783 - val_loss: 0.7306 - val_acc: 0.7870\n",
      "Epoch 75/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.6574 - acc: 0.7913 - val_loss: 0.7307 - val_acc: 0.7870\n",
      "Epoch 76/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.6556 - acc: 0.7932 - val_loss: 0.7297 - val_acc: 0.7830\n",
      "Epoch 77/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.6536 - acc: 0.7913 - val_loss: 0.7253 - val_acc: 0.7850\n",
      "Epoch 78/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.6503 - acc: 0.7903 - val_loss: 0.7250 - val_acc: 0.7870\n",
      "Epoch 79/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.6501 - acc: 0.7903 - val_loss: 0.7223 - val_acc: 0.7830\n",
      "Epoch 80/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.6465 - acc: 0.7942 - val_loss: 0.7189 - val_acc: 0.7850\n",
      "Epoch 81/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.6448 - acc: 0.7942 - val_loss: 0.7180 - val_acc: 0.7791\n",
      "Epoch 82/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.6436 - acc: 0.7932 - val_loss: 0.7176 - val_acc: 0.7830\n",
      "Epoch 83/200\n",
      "1006/1006 [==============================] - 0s 119us/step - loss: 0.6407 - acc: 0.7952 - val_loss: 0.7196 - val_acc: 0.7890\n",
      "Epoch 84/200\n",
      "1006/1006 [==============================] - 0s 160us/step - loss: 0.6398 - acc: 0.7952 - val_loss: 0.7145 - val_acc: 0.7850\n",
      "Epoch 85/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.6386 - acc: 0.7972 - val_loss: 0.7125 - val_acc: 0.7830\n",
      "Epoch 86/200\n",
      "1006/1006 [==============================] - 0s 144us/step - loss: 0.6354 - acc: 0.7992 - val_loss: 0.7134 - val_acc: 0.7771\n",
      "Epoch 87/200\n",
      "1006/1006 [==============================] - 0s 156us/step - loss: 0.6353 - acc: 0.7942 - val_loss: 0.7088 - val_acc: 0.7830\n",
      "Epoch 88/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.6327 - acc: 0.7962 - val_loss: 0.7098 - val_acc: 0.7850\n",
      "Epoch 89/200\n",
      "1006/1006 [==============================] - 0s 145us/step - loss: 0.6316 - acc: 0.7952 - val_loss: 0.7034 - val_acc: 0.7811\n",
      "Epoch 90/200\n",
      "1006/1006 [==============================] - 0s 147us/step - loss: 0.6291 - acc: 0.7982 - val_loss: 0.7048 - val_acc: 0.7890\n",
      "Epoch 91/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.6289 - acc: 0.7972 - val_loss: 0.7005 - val_acc: 0.7830\n",
      "Epoch 92/200\n",
      "1006/1006 [==============================] - 0s 135us/step - loss: 0.6271 - acc: 0.7982 - val_loss: 0.7012 - val_acc: 0.7850\n",
      "Epoch 93/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.6258 - acc: 0.7982 - val_loss: 0.6989 - val_acc: 0.7811\n",
      "Epoch 94/200\n",
      "1006/1006 [==============================] - 0s 138us/step - loss: 0.6233 - acc: 0.7972 - val_loss: 0.6962 - val_acc: 0.7811\n",
      "Epoch 95/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.6228 - acc: 0.7982 - val_loss: 0.6968 - val_acc: 0.7890\n",
      "Epoch 96/200\n",
      "1006/1006 [==============================] - 0s 124us/step - loss: 0.6212 - acc: 0.7982 - val_loss: 0.6971 - val_acc: 0.7830\n",
      "Epoch 97/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.6207 - acc: 0.7992 - val_loss: 0.6922 - val_acc: 0.7811\n",
      "Epoch 98/200\n",
      "1006/1006 [==============================] - 0s 147us/step - loss: 0.6194 - acc: 0.7972 - val_loss: 0.6917 - val_acc: 0.7791\n",
      "Epoch 99/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.6174 - acc: 0.8002 - val_loss: 0.6897 - val_acc: 0.7791\n",
      "Epoch 100/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.6164 - acc: 0.8012 - val_loss: 0.6886 - val_acc: 0.7771\n",
      "Epoch 101/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.6153 - acc: 0.8012 - val_loss: 0.6867 - val_acc: 0.7771\n",
      "Epoch 102/200\n",
      "1006/1006 [==============================] - 0s 127us/step - loss: 0.6139 - acc: 0.8002 - val_loss: 0.6876 - val_acc: 0.7830\n",
      "Epoch 103/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.6123 - acc: 0.7962 - val_loss: 0.6868 - val_acc: 0.7830\n",
      "Epoch 104/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.6115 - acc: 0.8002 - val_loss: 0.6845 - val_acc: 0.7830\n",
      "Epoch 105/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.6105 - acc: 0.8012 - val_loss: 0.6828 - val_acc: 0.7751\n",
      "Epoch 106/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.6081 - acc: 0.8012 - val_loss: 0.6839 - val_acc: 0.7830\n",
      "Epoch 107/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.6085 - acc: 0.8012 - val_loss: 0.6820 - val_acc: 0.7830\n",
      "Epoch 108/200\n",
      "1006/1006 [==============================] - 0s 106us/step - loss: 0.6065 - acc: 0.8022 - val_loss: 0.6807 - val_acc: 0.7811\n",
      "Epoch 109/200\n",
      "1006/1006 [==============================] - 0s 116us/step - loss: 0.6057 - acc: 0.8002 - val_loss: 0.6786 - val_acc: 0.7791\n",
      "Epoch 110/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.6043 - acc: 0.8072 - val_loss: 0.6782 - val_acc: 0.7712\n",
      "Epoch 111/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.6037 - acc: 0.8012 - val_loss: 0.6766 - val_acc: 0.7771\n",
      "Epoch 112/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.6019 - acc: 0.8052 - val_loss: 0.6763 - val_acc: 0.7811\n",
      "Epoch 113/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.6007 - acc: 0.8052 - val_loss: 0.6746 - val_acc: 0.7830\n",
      "Epoch 114/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.5996 - acc: 0.8062 - val_loss: 0.6749 - val_acc: 0.7771\n",
      "Epoch 115/200\n",
      "1006/1006 [==============================] - 0s 118us/step - loss: 0.5983 - acc: 0.8032 - val_loss: 0.6706 - val_acc: 0.7791\n",
      "Epoch 116/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.5979 - acc: 0.8032 - val_loss: 0.6696 - val_acc: 0.7791\n",
      "Epoch 117/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.5962 - acc: 0.8052 - val_loss: 0.6686 - val_acc: 0.7791\n",
      "Epoch 118/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.5940 - acc: 0.8032 - val_loss: 0.6704 - val_acc: 0.7811\n",
      "Epoch 119/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5947 - acc: 0.8012 - val_loss: 0.6660 - val_acc: 0.7732\n",
      "Epoch 120/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5932 - acc: 0.8052 - val_loss: 0.6654 - val_acc: 0.7771\n",
      "Epoch 121/200\n",
      "1006/1006 [==============================] - 0s 106us/step - loss: 0.5915 - acc: 0.8062 - val_loss: 0.6652 - val_acc: 0.7850\n",
      "Epoch 122/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5914 - acc: 0.8042 - val_loss: 0.6637 - val_acc: 0.7830\n",
      "Epoch 123/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5901 - acc: 0.8062 - val_loss: 0.6621 - val_acc: 0.7811\n",
      "Epoch 124/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5890 - acc: 0.8072 - val_loss: 0.6606 - val_acc: 0.7830\n",
      "Epoch 125/200\n",
      "1006/1006 [==============================] - 0s 105us/step - loss: 0.5874 - acc: 0.8042 - val_loss: 0.6641 - val_acc: 0.7870\n",
      "Epoch 126/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5873 - acc: 0.8052 - val_loss: 0.6605 - val_acc: 0.7771\n",
      "Epoch 127/200\n",
      "1006/1006 [==============================] - 0s 106us/step - loss: 0.5856 - acc: 0.8032 - val_loss: 0.6612 - val_acc: 0.7791\n",
      "Epoch 128/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5838 - acc: 0.8091 - val_loss: 0.6612 - val_acc: 0.7850\n",
      "Epoch 129/200\n",
      "1006/1006 [==============================] - 0s 106us/step - loss: 0.5841 - acc: 0.8022 - val_loss: 0.6592 - val_acc: 0.7791\n",
      "Epoch 130/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5822 - acc: 0.8082 - val_loss: 0.6560 - val_acc: 0.7890\n",
      "Epoch 131/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5815 - acc: 0.8052 - val_loss: 0.6568 - val_acc: 0.7890\n",
      "Epoch 132/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5811 - acc: 0.8052 - val_loss: 0.6564 - val_acc: 0.7870\n",
      "Epoch 133/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5794 - acc: 0.8082 - val_loss: 0.6561 - val_acc: 0.7870\n",
      "Epoch 134/200\n",
      "1006/1006 [==============================] - 0s 102us/step - loss: 0.5779 - acc: 0.8032 - val_loss: 0.6564 - val_acc: 0.7830\n",
      "Epoch 135/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5758 - acc: 0.8072 - val_loss: 0.6597 - val_acc: 0.7830\n",
      "Epoch 136/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5766 - acc: 0.8042 - val_loss: 0.6565 - val_acc: 0.7791\n",
      "Epoch 137/200\n",
      "1006/1006 [==============================] - ETA: 0s - loss: 0.5723 - acc: 0.810 - 0s 117us/step - loss: 0.5753 - acc: 0.8032 - val_loss: 0.6538 - val_acc: 0.7909\n",
      "Epoch 138/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5738 - acc: 0.8072 - val_loss: 0.6527 - val_acc: 0.7929\n",
      "Epoch 139/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5739 - acc: 0.8042 - val_loss: 0.6533 - val_acc: 0.7890\n",
      "Epoch 140/200\n",
      "1006/1006 [==============================] - 0s 103us/step - loss: 0.5721 - acc: 0.8052 - val_loss: 0.6529 - val_acc: 0.7890\n",
      "Epoch 141/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5709 - acc: 0.8082 - val_loss: 0.6522 - val_acc: 0.7929\n",
      "Epoch 142/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5707 - acc: 0.8091 - val_loss: 0.6509 - val_acc: 0.7909\n",
      "Epoch 143/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5706 - acc: 0.8091 - val_loss: 0.6501 - val_acc: 0.7830\n",
      "Epoch 144/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5698 - acc: 0.8111 - val_loss: 0.6507 - val_acc: 0.7909\n",
      "Epoch 145/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.5678 - acc: 0.8121 - val_loss: 0.6492 - val_acc: 0.7830\n",
      "Epoch 146/200\n",
      "1006/1006 [==============================] - 0s 102us/step - loss: 0.5678 - acc: 0.8082 - val_loss: 0.6511 - val_acc: 0.7850\n",
      "Epoch 147/200\n",
      "1006/1006 [==============================] - 0s 103us/step - loss: 0.5660 - acc: 0.8101 - val_loss: 0.6494 - val_acc: 0.7968\n",
      "Epoch 148/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5658 - acc: 0.8131 - val_loss: 0.6494 - val_acc: 0.7949\n",
      "Epoch 149/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5647 - acc: 0.8062 - val_loss: 0.6496 - val_acc: 0.7870\n",
      "Epoch 150/200\n",
      "1006/1006 [==============================] - 0s 108us/step - loss: 0.5637 - acc: 0.8091 - val_loss: 0.6478 - val_acc: 0.7929\n",
      "Epoch 151/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5626 - acc: 0.8151 - val_loss: 0.6477 - val_acc: 0.7929\n",
      "Epoch 152/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.5616 - acc: 0.8072 - val_loss: 0.6469 - val_acc: 0.7929\n",
      "Epoch 153/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.5614 - acc: 0.8121 - val_loss: 0.6466 - val_acc: 0.7968\n",
      "Epoch 154/200\n",
      "1006/1006 [==============================] - 0s 105us/step - loss: 0.5603 - acc: 0.8121 - val_loss: 0.6454 - val_acc: 0.7949\n",
      "Epoch 155/200\n",
      "1006/1006 [==============================] - 0s 109us/step - loss: 0.5600 - acc: 0.8131 - val_loss: 0.6462 - val_acc: 0.7968\n",
      "Epoch 156/200\n",
      "1006/1006 [==============================] - 0s 112us/step - loss: 0.5585 - acc: 0.8141 - val_loss: 0.6454 - val_acc: 0.7890\n",
      "Epoch 157/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.5576 - acc: 0.8101 - val_loss: 0.6452 - val_acc: 0.7949\n",
      "Epoch 158/200\n",
      "1006/1006 [==============================] - 0s 107us/step - loss: 0.5572 - acc: 0.8131 - val_loss: 0.6439 - val_acc: 0.7988\n",
      "Epoch 159/200\n",
      "1006/1006 [==============================] - 0s 110us/step - loss: 0.5563 - acc: 0.8101 - val_loss: 0.6438 - val_acc: 0.7949\n",
      "Epoch 160/200\n",
      "1006/1006 [==============================] - 0s 128us/step - loss: 0.5548 - acc: 0.8111 - val_loss: 0.6455 - val_acc: 0.7988\n",
      "Epoch 161/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.5539 - acc: 0.8131 - val_loss: 0.6458 - val_acc: 0.7968\n",
      "Epoch 162/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.5536 - acc: 0.8111 - val_loss: 0.6444 - val_acc: 0.7909\n",
      "Epoch 163/200\n",
      "1006/1006 [==============================] - 0s 154us/step - loss: 0.5519 - acc: 0.8131 - val_loss: 0.6433 - val_acc: 0.7968\n",
      "Epoch 164/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.5511 - acc: 0.8121 - val_loss: 0.6435 - val_acc: 0.7929\n",
      "Epoch 165/200\n",
      "1006/1006 [==============================] - 0s 139us/step - loss: 0.5514 - acc: 0.8151 - val_loss: 0.6428 - val_acc: 0.8008\n",
      "Epoch 166/200\n",
      "1006/1006 [==============================] - 0s 129us/step - loss: 0.5511 - acc: 0.8141 - val_loss: 0.6430 - val_acc: 0.7988\n",
      "Epoch 167/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.5491 - acc: 0.8091 - val_loss: 0.6422 - val_acc: 0.7988\n",
      "Epoch 168/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.5490 - acc: 0.8171 - val_loss: 0.6425 - val_acc: 0.7988\n",
      "Epoch 169/200\n",
      "1006/1006 [==============================] - 0s 115us/step - loss: 0.5480 - acc: 0.8141 - val_loss: 0.6400 - val_acc: 0.7988\n",
      "Epoch 170/200\n",
      "1006/1006 [==============================] - 0s 123us/step - loss: 0.5470 - acc: 0.8181 - val_loss: 0.6379 - val_acc: 0.8008\n",
      "Epoch 171/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.5467 - acc: 0.8161 - val_loss: 0.6399 - val_acc: 0.7968\n",
      "Epoch 172/200\n",
      "1006/1006 [==============================] - 0s 122us/step - loss: 0.5455 - acc: 0.8141 - val_loss: 0.6392 - val_acc: 0.8008\n",
      "Epoch 173/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.5443 - acc: 0.8141 - val_loss: 0.6372 - val_acc: 0.8008\n",
      "Epoch 174/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.5424 - acc: 0.8181 - val_loss: 0.6415 - val_acc: 0.7988\n",
      "Epoch 175/200\n",
      "1006/1006 [==============================] - 0s 131us/step - loss: 0.5440 - acc: 0.8161 - val_loss: 0.6368 - val_acc: 0.8028\n",
      "Epoch 176/200\n",
      "1006/1006 [==============================] - 0s 150us/step - loss: 0.5418 - acc: 0.8201 - val_loss: 0.6370 - val_acc: 0.8008\n",
      "Epoch 177/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.5416 - acc: 0.8211 - val_loss: 0.6379 - val_acc: 0.8008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "1006/1006 [==============================] - 0s 119us/step - loss: 0.5406 - acc: 0.8181 - val_loss: 0.6369 - val_acc: 0.8008\n",
      "Epoch 179/200\n",
      "1006/1006 [==============================] - 0s 113us/step - loss: 0.5400 - acc: 0.8191 - val_loss: 0.6355 - val_acc: 0.8028\n",
      "Epoch 180/200\n",
      "1006/1006 [==============================] - 0s 134us/step - loss: 0.5388 - acc: 0.8191 - val_loss: 0.6345 - val_acc: 0.8028\n",
      "Epoch 181/200\n",
      "1006/1006 [==============================] - 0s 149us/step - loss: 0.5395 - acc: 0.8211 - val_loss: 0.6350 - val_acc: 0.8008\n",
      "Epoch 182/200\n",
      "1006/1006 [==============================] - 0s 136us/step - loss: 0.5383 - acc: 0.8171 - val_loss: 0.6352 - val_acc: 0.7968\n",
      "Epoch 183/200\n",
      "1006/1006 [==============================] - 0s 125us/step - loss: 0.5373 - acc: 0.8191 - val_loss: 0.6349 - val_acc: 0.7988\n",
      "Epoch 184/200\n",
      "1006/1006 [==============================] - 0s 156us/step - loss: 0.5367 - acc: 0.8201 - val_loss: 0.6359 - val_acc: 0.7988\n",
      "Epoch 185/200\n",
      "1006/1006 [==============================] - 0s 143us/step - loss: 0.5354 - acc: 0.8221 - val_loss: 0.6335 - val_acc: 0.8008\n",
      "Epoch 186/200\n",
      "1006/1006 [==============================] - 0s 126us/step - loss: 0.5342 - acc: 0.8171 - val_loss: 0.6347 - val_acc: 0.7968\n",
      "Epoch 187/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.5342 - acc: 0.8241 - val_loss: 0.6346 - val_acc: 0.7968\n",
      "Epoch 188/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.5334 - acc: 0.8260 - val_loss: 0.6323 - val_acc: 0.8008\n",
      "Epoch 189/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.5328 - acc: 0.8171 - val_loss: 0.6326 - val_acc: 0.8008\n",
      "Epoch 190/200\n",
      "1006/1006 [==============================] - 0s 111us/step - loss: 0.5324 - acc: 0.8191 - val_loss: 0.6325 - val_acc: 0.7968\n",
      "Epoch 191/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 0.5311 - acc: 0.8211 - val_loss: 0.6362 - val_acc: 0.7968\n",
      "Epoch 192/200\n",
      "1006/1006 [==============================] - 0s 146us/step - loss: 0.5309 - acc: 0.8241 - val_loss: 0.6335 - val_acc: 0.8028\n",
      "Epoch 193/200\n",
      "1006/1006 [==============================] - 0s 140us/step - loss: 0.5304 - acc: 0.8211 - val_loss: 0.6301 - val_acc: 0.7988\n",
      "Epoch 194/200\n",
      "1006/1006 [==============================] - 0s 124us/step - loss: 0.5294 - acc: 0.8221 - val_loss: 0.6299 - val_acc: 0.8008\n",
      "Epoch 195/200\n",
      "1006/1006 [==============================] - 0s 133us/step - loss: 0.5278 - acc: 0.8231 - val_loss: 0.6298 - val_acc: 0.7988\n",
      "Epoch 196/200\n",
      "1006/1006 [==============================] - 0s 152us/step - loss: 0.5278 - acc: 0.8221 - val_loss: 0.6280 - val_acc: 0.8028\n",
      "Epoch 197/200\n",
      "1006/1006 [==============================] - 0s 142us/step - loss: 0.5272 - acc: 0.8231 - val_loss: 0.6290 - val_acc: 0.8008\n",
      "Epoch 198/200\n",
      "1006/1006 [==============================] - 0s 114us/step - loss: 0.5258 - acc: 0.8191 - val_loss: 0.6312 - val_acc: 0.7968\n",
      "Epoch 199/200\n",
      "1006/1006 [==============================] - 0s 148us/step - loss: 0.5253 - acc: 0.8231 - val_loss: 0.6289 - val_acc: 0.8008\n",
      "Epoch 200/200\n",
      "1006/1006 [==============================] - 0s 145us/step - loss: 0.5249 - acc: 0.8250 - val_loss: 0.6288 - val_acc: 0.8008\n",
      "Train on 1007 samples, validate on 506 samples\n",
      "Epoch 1/200\n",
      "1007/1007 [==============================] - 0s 140us/step - loss: 0.5752 - acc: 0.8103 - val_loss: 0.5299 - val_acc: 0.8221\n",
      "Epoch 2/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.5717 - acc: 0.8113 - val_loss: 0.5295 - val_acc: 0.8182\n",
      "Epoch 3/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.5694 - acc: 0.8093 - val_loss: 0.5335 - val_acc: 0.8241\n",
      "Epoch 4/200\n",
      "1007/1007 [==============================] - 0s 153us/step - loss: 0.5677 - acc: 0.8113 - val_loss: 0.5359 - val_acc: 0.8221\n",
      "Epoch 5/200\n",
      "1007/1007 [==============================] - 0s 137us/step - loss: 0.5663 - acc: 0.8143 - val_loss: 0.5355 - val_acc: 0.8241\n",
      "Epoch 6/200\n",
      "1007/1007 [==============================] - 0s 118us/step - loss: 0.5644 - acc: 0.8093 - val_loss: 0.5368 - val_acc: 0.8182\n",
      "Epoch 7/200\n",
      "1007/1007 [==============================] - 0s 149us/step - loss: 0.5631 - acc: 0.8133 - val_loss: 0.5366 - val_acc: 0.8221\n",
      "Epoch 8/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.5619 - acc: 0.8133 - val_loss: 0.5377 - val_acc: 0.8221\n",
      "Epoch 9/200\n",
      "1007/1007 [==============================] - 0s 130us/step - loss: 0.5611 - acc: 0.8123 - val_loss: 0.5398 - val_acc: 0.8221\n",
      "Epoch 10/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.5595 - acc: 0.8153 - val_loss: 0.5390 - val_acc: 0.8221\n",
      "Epoch 11/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.5580 - acc: 0.8093 - val_loss: 0.5433 - val_acc: 0.8221\n",
      "Epoch 12/200\n",
      "1007/1007 [==============================] - 0s 154us/step - loss: 0.5583 - acc: 0.8143 - val_loss: 0.5419 - val_acc: 0.8241\n",
      "Epoch 13/200\n",
      "1007/1007 [==============================] - 0s 139us/step - loss: 0.5553 - acc: 0.8123 - val_loss: 0.5454 - val_acc: 0.8162\n",
      "Epoch 14/200\n",
      "1007/1007 [==============================] - 0s 145us/step - loss: 0.5556 - acc: 0.8123 - val_loss: 0.5451 - val_acc: 0.8202\n",
      "Epoch 15/200\n",
      "1007/1007 [==============================] - 0s 152us/step - loss: 0.5541 - acc: 0.8143 - val_loss: 0.5452 - val_acc: 0.8182\n",
      "Epoch 16/200\n",
      "1007/1007 [==============================] - 0s 143us/step - loss: 0.5531 - acc: 0.8133 - val_loss: 0.5483 - val_acc: 0.8202\n",
      "Epoch 17/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.5529 - acc: 0.8133 - val_loss: 0.5492 - val_acc: 0.8162\n",
      "Epoch 18/200\n",
      "1007/1007 [==============================] - 0s 139us/step - loss: 0.5519 - acc: 0.8153 - val_loss: 0.5482 - val_acc: 0.8182\n",
      "Epoch 19/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5495 - acc: 0.8153 - val_loss: 0.5530 - val_acc: 0.8162\n",
      "Epoch 20/200\n",
      "1007/1007 [==============================] - 0s 131us/step - loss: 0.5500 - acc: 0.8173 - val_loss: 0.5527 - val_acc: 0.8123\n",
      "Epoch 21/200\n",
      "1007/1007 [==============================] - 0s 133us/step - loss: 0.5500 - acc: 0.8123 - val_loss: 0.5504 - val_acc: 0.8182\n",
      "Epoch 22/200\n",
      "1007/1007 [==============================] - 0s 157us/step - loss: 0.5480 - acc: 0.8153 - val_loss: 0.5485 - val_acc: 0.8221\n",
      "Epoch 23/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.5476 - acc: 0.8123 - val_loss: 0.5498 - val_acc: 0.8221\n",
      "Epoch 24/200\n",
      "1007/1007 [==============================] - 0s 115us/step - loss: 0.5466 - acc: 0.8143 - val_loss: 0.5524 - val_acc: 0.8182\n",
      "Epoch 25/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.5453 - acc: 0.8143 - val_loss: 0.5535 - val_acc: 0.8221\n",
      "Epoch 26/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.5451 - acc: 0.8143 - val_loss: 0.5535 - val_acc: 0.8221\n",
      "Epoch 27/200\n",
      "1007/1007 [==============================] - 0s 122us/step - loss: 0.5440 - acc: 0.8123 - val_loss: 0.5516 - val_acc: 0.8221\n",
      "Epoch 28/200\n",
      "1007/1007 [==============================] - 0s 125us/step - loss: 0.5420 - acc: 0.8113 - val_loss: 0.5596 - val_acc: 0.8162\n",
      "Epoch 29/200\n",
      "1007/1007 [==============================] - 0s 125us/step - loss: 0.5438 - acc: 0.8153 - val_loss: 0.5527 - val_acc: 0.8241\n",
      "Epoch 30/200\n",
      "1007/1007 [==============================] - 0s 127us/step - loss: 0.5420 - acc: 0.8163 - val_loss: 0.5550 - val_acc: 0.8202\n",
      "Epoch 31/200\n",
      "1007/1007 [==============================] - 0s 117us/step - loss: 0.5415 - acc: 0.8213 - val_loss: 0.5557 - val_acc: 0.8202\n",
      "Epoch 32/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.5414 - acc: 0.8143 - val_loss: 0.5527 - val_acc: 0.8241\n",
      "Epoch 33/200\n",
      "1007/1007 [==============================] - 0s 137us/step - loss: 0.5397 - acc: 0.8133 - val_loss: 0.5545 - val_acc: 0.8221\n",
      "Epoch 34/200\n",
      "1007/1007 [==============================] - 0s 161us/step - loss: 0.5397 - acc: 0.8153 - val_loss: 0.5555 - val_acc: 0.8241\n",
      "Epoch 35/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.5386 - acc: 0.8203 - val_loss: 0.5539 - val_acc: 0.8221\n",
      "Epoch 36/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.5385 - acc: 0.8183 - val_loss: 0.5550 - val_acc: 0.8221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "1007/1007 [==============================] - 0s 134us/step - loss: 0.5363 - acc: 0.8163 - val_loss: 0.5534 - val_acc: 0.8281\n",
      "Epoch 38/200\n",
      "1007/1007 [==============================] - 0s 143us/step - loss: 0.5358 - acc: 0.8133 - val_loss: 0.5564 - val_acc: 0.8202\n",
      "Epoch 39/200\n",
      "1007/1007 [==============================] - 0s 144us/step - loss: 0.5362 - acc: 0.8173 - val_loss: 0.5590 - val_acc: 0.8162\n",
      "Epoch 40/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.5355 - acc: 0.8193 - val_loss: 0.5609 - val_acc: 0.8221\n",
      "Epoch 41/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.5357 - acc: 0.8153 - val_loss: 0.5562 - val_acc: 0.8221\n",
      "Epoch 42/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5350 - acc: 0.8173 - val_loss: 0.5568 - val_acc: 0.8221\n",
      "Epoch 43/200\n",
      "1007/1007 [==============================] - 0s 144us/step - loss: 0.5342 - acc: 0.8133 - val_loss: 0.5545 - val_acc: 0.8241\n",
      "Epoch 44/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.5326 - acc: 0.8133 - val_loss: 0.5559 - val_acc: 0.8221\n",
      "Epoch 45/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.5331 - acc: 0.8143 - val_loss: 0.5569 - val_acc: 0.8182\n",
      "Epoch 46/200\n",
      "1007/1007 [==============================] - 0s 125us/step - loss: 0.5317 - acc: 0.8153 - val_loss: 0.5564 - val_acc: 0.8241\n",
      "Epoch 47/200\n",
      "1007/1007 [==============================] - 0s 115us/step - loss: 0.5314 - acc: 0.8163 - val_loss: 0.5582 - val_acc: 0.8261\n",
      "Epoch 48/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.5313 - acc: 0.8173 - val_loss: 0.5558 - val_acc: 0.8202\n",
      "Epoch 49/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.5294 - acc: 0.8183 - val_loss: 0.5575 - val_acc: 0.8261\n",
      "Epoch 50/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.5283 - acc: 0.8193 - val_loss: 0.5562 - val_acc: 0.8142\n",
      "Epoch 51/200\n",
      "1007/1007 [==============================] - 0s 137us/step - loss: 0.5286 - acc: 0.8143 - val_loss: 0.5603 - val_acc: 0.8221\n",
      "Epoch 52/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.5288 - acc: 0.8183 - val_loss: 0.5598 - val_acc: 0.8202\n",
      "Epoch 53/200\n",
      "1007/1007 [==============================] - 0s 136us/step - loss: 0.5286 - acc: 0.8183 - val_loss: 0.5566 - val_acc: 0.8261\n",
      "Epoch 54/200\n",
      "1007/1007 [==============================] - 0s 114us/step - loss: 0.5272 - acc: 0.8193 - val_loss: 0.5568 - val_acc: 0.8202\n",
      "Epoch 55/200\n",
      "1007/1007 [==============================] - 0s 142us/step - loss: 0.5267 - acc: 0.8153 - val_loss: 0.5568 - val_acc: 0.8241\n",
      "Epoch 56/200\n",
      "1007/1007 [==============================] - 0s 142us/step - loss: 0.5256 - acc: 0.8193 - val_loss: 0.5574 - val_acc: 0.8202\n",
      "Epoch 57/200\n",
      "1007/1007 [==============================] - 0s 131us/step - loss: 0.5255 - acc: 0.8163 - val_loss: 0.5588 - val_acc: 0.8221\n",
      "Epoch 58/200\n",
      "1007/1007 [==============================] - 0s 142us/step - loss: 0.5239 - acc: 0.8163 - val_loss: 0.5573 - val_acc: 0.8221\n",
      "Epoch 59/200\n",
      "1007/1007 [==============================] - 0s 157us/step - loss: 0.5249 - acc: 0.8183 - val_loss: 0.5603 - val_acc: 0.8221\n",
      "Epoch 60/200\n",
      "1007/1007 [==============================] - 0s 140us/step - loss: 0.5243 - acc: 0.8173 - val_loss: 0.5570 - val_acc: 0.8182\n",
      "Epoch 61/200\n",
      "1007/1007 [==============================] - 0s 130us/step - loss: 0.5230 - acc: 0.8203 - val_loss: 0.5576 - val_acc: 0.8182\n",
      "Epoch 62/200\n",
      "1007/1007 [==============================] - 0s 155us/step - loss: 0.5219 - acc: 0.8183 - val_loss: 0.5584 - val_acc: 0.8241\n",
      "Epoch 63/200\n",
      "1007/1007 [==============================] - 0s 145us/step - loss: 0.5225 - acc: 0.8183 - val_loss: 0.5600 - val_acc: 0.8182\n",
      "Epoch 64/200\n",
      "1007/1007 [==============================] - 0s 132us/step - loss: 0.5217 - acc: 0.8213 - val_loss: 0.5567 - val_acc: 0.8202\n",
      "Epoch 65/200\n",
      "1007/1007 [==============================] - 0s 151us/step - loss: 0.5208 - acc: 0.8222 - val_loss: 0.5591 - val_acc: 0.8241\n",
      "Epoch 66/200\n",
      "1007/1007 [==============================] - 0s 152us/step - loss: 0.5198 - acc: 0.8183 - val_loss: 0.5622 - val_acc: 0.8202\n",
      "Epoch 67/200\n",
      "1007/1007 [==============================] - 0s 136us/step - loss: 0.5201 - acc: 0.8193 - val_loss: 0.5590 - val_acc: 0.8241\n",
      "Epoch 68/200\n",
      "1007/1007 [==============================] - 0s 117us/step - loss: 0.5178 - acc: 0.8222 - val_loss: 0.5594 - val_acc: 0.8221\n",
      "Epoch 69/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.5191 - acc: 0.8242 - val_loss: 0.5592 - val_acc: 0.8202\n",
      "Epoch 70/200\n",
      "1007/1007 [==============================] - 0s 149us/step - loss: 0.5187 - acc: 0.8193 - val_loss: 0.5616 - val_acc: 0.8202\n",
      "Epoch 71/200\n",
      "1007/1007 [==============================] - 0s 115us/step - loss: 0.5181 - acc: 0.8213 - val_loss: 0.5599 - val_acc: 0.8221\n",
      "Epoch 72/200\n",
      "1007/1007 [==============================] - 0s 130us/step - loss: 0.5151 - acc: 0.8193 - val_loss: 0.5639 - val_acc: 0.8261\n",
      "Epoch 73/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5170 - acc: 0.8183 - val_loss: 0.5623 - val_acc: 0.8241\n",
      "Epoch 74/200\n",
      "1007/1007 [==============================] - 0s 157us/step - loss: 0.5152 - acc: 0.8262 - val_loss: 0.5591 - val_acc: 0.8221\n",
      "Epoch 75/200\n",
      "1007/1007 [==============================] - 0s 128us/step - loss: 0.5156 - acc: 0.8213 - val_loss: 0.5599 - val_acc: 0.8221\n",
      "Epoch 76/200\n",
      "1007/1007 [==============================] - ETA: 0s - loss: 0.5028 - acc: 0.821 - 0s 150us/step - loss: 0.5147 - acc: 0.8193 - val_loss: 0.5613 - val_acc: 0.8162\n",
      "Epoch 77/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.5138 - acc: 0.8203 - val_loss: 0.5587 - val_acc: 0.8221\n",
      "Epoch 78/200\n",
      "1007/1007 [==============================] - 0s 146us/step - loss: 0.5146 - acc: 0.8232 - val_loss: 0.5610 - val_acc: 0.8241\n",
      "Epoch 79/200\n",
      "1007/1007 [==============================] - 0s 131us/step - loss: 0.5136 - acc: 0.8232 - val_loss: 0.5614 - val_acc: 0.8202\n",
      "Epoch 80/200\n",
      "1007/1007 [==============================] - 0s 122us/step - loss: 0.5119 - acc: 0.8232 - val_loss: 0.5589 - val_acc: 0.8202\n",
      "Epoch 81/200\n",
      "1007/1007 [==============================] - 0s 145us/step - loss: 0.5124 - acc: 0.8183 - val_loss: 0.5590 - val_acc: 0.8202\n",
      "Epoch 82/200\n",
      "1007/1007 [==============================] - 0s 149us/step - loss: 0.5113 - acc: 0.8222 - val_loss: 0.5588 - val_acc: 0.8221\n",
      "Epoch 83/200\n",
      "1007/1007 [==============================] - 0s 132us/step - loss: 0.5106 - acc: 0.8222 - val_loss: 0.5619 - val_acc: 0.8142\n",
      "Epoch 84/200\n",
      "1007/1007 [==============================] - 0s 130us/step - loss: 0.5114 - acc: 0.8242 - val_loss: 0.5601 - val_acc: 0.8202\n",
      "Epoch 85/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.5108 - acc: 0.8242 - val_loss: 0.5603 - val_acc: 0.8221\n",
      "Epoch 86/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.5088 - acc: 0.8222 - val_loss: 0.5631 - val_acc: 0.8162\n",
      "Epoch 87/200\n",
      "1007/1007 [==============================] - 0s 132us/step - loss: 0.5091 - acc: 0.8252 - val_loss: 0.5631 - val_acc: 0.8202\n",
      "Epoch 88/200\n",
      "1007/1007 [==============================] - 0s 140us/step - loss: 0.5088 - acc: 0.8232 - val_loss: 0.5635 - val_acc: 0.8162\n",
      "Epoch 89/200\n",
      "1007/1007 [==============================] - 0s 165us/step - loss: 0.5087 - acc: 0.8262 - val_loss: 0.5624 - val_acc: 0.8162\n",
      "Epoch 90/200\n",
      "1007/1007 [==============================] - 0s 161us/step - loss: 0.5064 - acc: 0.8262 - val_loss: 0.5667 - val_acc: 0.8162\n",
      "Epoch 91/200\n",
      "1007/1007 [==============================] - 0s 160us/step - loss: 0.5085 - acc: 0.8232 - val_loss: 0.5610 - val_acc: 0.8202\n",
      "Epoch 92/200\n",
      "1007/1007 [==============================] - 0s 161us/step - loss: 0.5068 - acc: 0.8282 - val_loss: 0.5623 - val_acc: 0.8241\n",
      "Epoch 93/200\n",
      "1007/1007 [==============================] - 0s 158us/step - loss: 0.5064 - acc: 0.8252 - val_loss: 0.5619 - val_acc: 0.8221\n",
      "Epoch 94/200\n",
      "1007/1007 [==============================] - 0s 166us/step - loss: 0.5061 - acc: 0.8213 - val_loss: 0.5628 - val_acc: 0.8221\n",
      "Epoch 95/200\n",
      "1007/1007 [==============================] - 0s 168us/step - loss: 0.5055 - acc: 0.8262 - val_loss: 0.5610 - val_acc: 0.8182\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5038 - acc: 0.8242 - val_loss: 0.5639 - val_acc: 0.8123\n",
      "Epoch 97/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.5037 - acc: 0.8252 - val_loss: 0.5650 - val_acc: 0.8123\n",
      "Epoch 98/200\n",
      "1007/1007 [==============================] - 0s 145us/step - loss: 0.5050 - acc: 0.8232 - val_loss: 0.5631 - val_acc: 0.8142\n",
      "Epoch 99/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5032 - acc: 0.8213 - val_loss: 0.5605 - val_acc: 0.8182\n",
      "Epoch 100/200\n",
      "1007/1007 [==============================] - 0s 133us/step - loss: 0.5030 - acc: 0.8282 - val_loss: 0.5615 - val_acc: 0.8202\n",
      "Epoch 101/200\n",
      "1007/1007 [==============================] - 0s 106us/step - loss: 0.5033 - acc: 0.8213 - val_loss: 0.5616 - val_acc: 0.8162\n",
      "Epoch 102/200\n",
      "1007/1007 [==============================] - 0s 124us/step - loss: 0.5017 - acc: 0.8262 - val_loss: 0.5599 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.5013 - acc: 0.8242 - val_loss: 0.5625 - val_acc: 0.8162\n",
      "Epoch 104/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.5014 - acc: 0.8222 - val_loss: 0.5602 - val_acc: 0.8221\n",
      "Epoch 105/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.5014 - acc: 0.8252 - val_loss: 0.5618 - val_acc: 0.8182\n",
      "Epoch 106/200\n",
      "1007/1007 [==============================] - 0s 151us/step - loss: 0.5000 - acc: 0.8203 - val_loss: 0.5588 - val_acc: 0.8281\n",
      "Epoch 107/200\n",
      "1007/1007 [==============================] - 0s 144us/step - loss: 0.5002 - acc: 0.8252 - val_loss: 0.5618 - val_acc: 0.8162\n",
      "Epoch 108/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.4994 - acc: 0.8302 - val_loss: 0.5637 - val_acc: 0.8182\n",
      "Epoch 109/200\n",
      "1007/1007 [==============================] - 0s 151us/step - loss: 0.4987 - acc: 0.8213 - val_loss: 0.5641 - val_acc: 0.8123\n",
      "Epoch 110/200\n",
      "1007/1007 [==============================] - 0s 126us/step - loss: 0.4982 - acc: 0.8252 - val_loss: 0.5626 - val_acc: 0.8221\n",
      "Epoch 111/200\n",
      "1007/1007 [==============================] - 0s 134us/step - loss: 0.4983 - acc: 0.8252 - val_loss: 0.5614 - val_acc: 0.8261\n",
      "Epoch 112/200\n",
      "1007/1007 [==============================] - 0s 130us/step - loss: 0.4980 - acc: 0.8282 - val_loss: 0.5617 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "1007/1007 [==============================] - 0s 127us/step - loss: 0.4962 - acc: 0.8242 - val_loss: 0.5635 - val_acc: 0.8300\n",
      "Epoch 114/200\n",
      "1007/1007 [==============================] - 0s 163us/step - loss: 0.4966 - acc: 0.8272 - val_loss: 0.5633 - val_acc: 0.8202\n",
      "Epoch 115/200\n",
      "1007/1007 [==============================] - 0s 159us/step - loss: 0.4960 - acc: 0.8252 - val_loss: 0.5640 - val_acc: 0.8281\n",
      "Epoch 116/200\n",
      "1007/1007 [==============================] - 0s 158us/step - loss: 0.4967 - acc: 0.8292 - val_loss: 0.5630 - val_acc: 0.8202\n",
      "Epoch 117/200\n",
      "1007/1007 [==============================] - 0s 155us/step - loss: 0.4961 - acc: 0.8213 - val_loss: 0.5636 - val_acc: 0.8202\n",
      "Epoch 118/200\n",
      "1007/1007 [==============================] - 0s 142us/step - loss: 0.4954 - acc: 0.8252 - val_loss: 0.5637 - val_acc: 0.8202\n",
      "Epoch 119/200\n",
      "1007/1007 [==============================] - 0s 164us/step - loss: 0.4941 - acc: 0.8232 - val_loss: 0.5650 - val_acc: 0.8221\n",
      "Epoch 120/200\n",
      "1007/1007 [==============================] - 0s 163us/step - loss: 0.4951 - acc: 0.8282 - val_loss: 0.5616 - val_acc: 0.8182\n",
      "Epoch 121/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.4937 - acc: 0.8262 - val_loss: 0.5630 - val_acc: 0.8221\n",
      "Epoch 122/200\n",
      "1007/1007 [==============================] - 0s 153us/step - loss: 0.4932 - acc: 0.8262 - val_loss: 0.5636 - val_acc: 0.8182\n",
      "Epoch 123/200\n",
      "1007/1007 [==============================] - 0s 154us/step - loss: 0.4937 - acc: 0.8292 - val_loss: 0.5605 - val_acc: 0.8300\n",
      "Epoch 124/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.4917 - acc: 0.8232 - val_loss: 0.5644 - val_acc: 0.8142\n",
      "Epoch 125/200\n",
      "1007/1007 [==============================] - 0s 158us/step - loss: 0.4921 - acc: 0.8282 - val_loss: 0.5648 - val_acc: 0.8221\n",
      "Epoch 126/200\n",
      "1007/1007 [==============================] - 0s 156us/step - loss: 0.4912 - acc: 0.8302 - val_loss: 0.5674 - val_acc: 0.8182\n",
      "Epoch 127/200\n",
      "1007/1007 [==============================] - 0s 160us/step - loss: 0.4916 - acc: 0.8292 - val_loss: 0.5619 - val_acc: 0.8281\n",
      "Epoch 128/200\n",
      "1007/1007 [==============================] - 0s 152us/step - loss: 0.4901 - acc: 0.8322 - val_loss: 0.5649 - val_acc: 0.8123\n",
      "Epoch 129/200\n",
      "1007/1007 [==============================] - 0s 158us/step - loss: 0.4912 - acc: 0.8282 - val_loss: 0.5617 - val_acc: 0.8221\n",
      "Epoch 130/200\n",
      "1007/1007 [==============================] - 0s 115us/step - loss: 0.4894 - acc: 0.8332 - val_loss: 0.5641 - val_acc: 0.8123\n",
      "Epoch 131/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4906 - acc: 0.8312 - val_loss: 0.5612 - val_acc: 0.8182\n",
      "Epoch 132/200\n",
      "1007/1007 [==============================] - 0s 126us/step - loss: 0.4897 - acc: 0.8332 - val_loss: 0.5615 - val_acc: 0.8241\n",
      "Epoch 133/200\n",
      "1007/1007 [==============================] - 0s 133us/step - loss: 0.4890 - acc: 0.8302 - val_loss: 0.5635 - val_acc: 0.8202\n",
      "Epoch 134/200\n",
      "1007/1007 [==============================] - 0s 128us/step - loss: 0.4886 - acc: 0.8322 - val_loss: 0.5623 - val_acc: 0.8221\n",
      "Epoch 135/200\n",
      "1007/1007 [==============================] - 0s 135us/step - loss: 0.4878 - acc: 0.8322 - val_loss: 0.5611 - val_acc: 0.8241\n",
      "Epoch 136/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.4875 - acc: 0.8332 - val_loss: 0.5610 - val_acc: 0.8281\n",
      "Epoch 137/200\n",
      "1007/1007 [==============================] - 0s 125us/step - loss: 0.4863 - acc: 0.8292 - val_loss: 0.5629 - val_acc: 0.8162\n",
      "Epoch 138/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.4873 - acc: 0.8292 - val_loss: 0.5611 - val_acc: 0.8182\n",
      "Epoch 139/200\n",
      "1007/1007 [==============================] - 0s 131us/step - loss: 0.4866 - acc: 0.8352 - val_loss: 0.5607 - val_acc: 0.8241\n",
      "Epoch 140/200\n",
      "1007/1007 [==============================] - 0s 128us/step - loss: 0.4858 - acc: 0.8272 - val_loss: 0.5638 - val_acc: 0.8182\n",
      "Epoch 141/200\n",
      "1007/1007 [==============================] - 0s 159us/step - loss: 0.4847 - acc: 0.8302 - val_loss: 0.5651 - val_acc: 0.8142\n",
      "Epoch 142/200\n",
      "1007/1007 [==============================] - 0s 153us/step - loss: 0.4851 - acc: 0.8322 - val_loss: 0.5605 - val_acc: 0.8281\n",
      "Epoch 143/200\n",
      "1007/1007 [==============================] - 0s 161us/step - loss: 0.4852 - acc: 0.8272 - val_loss: 0.5615 - val_acc: 0.8221\n",
      "Epoch 144/200\n",
      "1007/1007 [==============================] - 0s 152us/step - loss: 0.4842 - acc: 0.8312 - val_loss: 0.5609 - val_acc: 0.8360\n",
      "Epoch 145/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.4828 - acc: 0.8302 - val_loss: 0.5655 - val_acc: 0.8221\n",
      "Epoch 146/200\n",
      "1007/1007 [==============================] - 0s 153us/step - loss: 0.4841 - acc: 0.8342 - val_loss: 0.5615 - val_acc: 0.8281\n",
      "Epoch 147/200\n",
      "1007/1007 [==============================] - 0s 157us/step - loss: 0.4825 - acc: 0.8272 - val_loss: 0.5624 - val_acc: 0.8221\n",
      "Epoch 148/200\n",
      "1007/1007 [==============================] - 0s 163us/step - loss: 0.4829 - acc: 0.8352 - val_loss: 0.5595 - val_acc: 0.8281\n",
      "Epoch 149/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.4826 - acc: 0.8292 - val_loss: 0.5634 - val_acc: 0.8182\n",
      "Epoch 150/200\n",
      "1007/1007 [==============================] - 0s 116us/step - loss: 0.4815 - acc: 0.8292 - val_loss: 0.5664 - val_acc: 0.8281\n",
      "Epoch 151/200\n",
      "1007/1007 [==============================] - 0s 147us/step - loss: 0.4816 - acc: 0.8282 - val_loss: 0.5610 - val_acc: 0.8320\n",
      "Epoch 152/200\n",
      "1007/1007 [==============================] - 0s 159us/step - loss: 0.4814 - acc: 0.8342 - val_loss: 0.5609 - val_acc: 0.8320\n",
      "Epoch 153/200\n",
      "1007/1007 [==============================] - 0s 150us/step - loss: 0.4801 - acc: 0.8352 - val_loss: 0.5603 - val_acc: 0.8281\n",
      "Epoch 154/200\n",
      "1007/1007 [==============================] - 0s 153us/step - loss: 0.4802 - acc: 0.8322 - val_loss: 0.5627 - val_acc: 0.8162\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 [==============================] - 0s 152us/step - loss: 0.4800 - acc: 0.8332 - val_loss: 0.5634 - val_acc: 0.8123\n",
      "Epoch 156/200\n",
      "1007/1007 [==============================] - 0s 148us/step - loss: 0.4800 - acc: 0.8312 - val_loss: 0.5615 - val_acc: 0.8281\n",
      "Epoch 157/200\n",
      "1007/1007 [==============================] - 0s 143us/step - loss: 0.4782 - acc: 0.8391 - val_loss: 0.5642 - val_acc: 0.8182\n",
      "Epoch 158/200\n",
      "1007/1007 [==============================] - 0s 107us/step - loss: 0.4776 - acc: 0.8332 - val_loss: 0.5630 - val_acc: 0.8241\n",
      "Epoch 159/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.4788 - acc: 0.8302 - val_loss: 0.5597 - val_acc: 0.8320\n",
      "Epoch 160/200\n",
      "1007/1007 [==============================] - 0s 108us/step - loss: 0.4766 - acc: 0.8352 - val_loss: 0.5638 - val_acc: 0.8202\n",
      "Epoch 161/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.4777 - acc: 0.8332 - val_loss: 0.5606 - val_acc: 0.8281\n",
      "Epoch 162/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4756 - acc: 0.8342 - val_loss: 0.5646 - val_acc: 0.8142\n",
      "Epoch 163/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4773 - acc: 0.8401 - val_loss: 0.5609 - val_acc: 0.8340\n",
      "Epoch 164/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4761 - acc: 0.8381 - val_loss: 0.5646 - val_acc: 0.8221\n",
      "Epoch 165/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4765 - acc: 0.8361 - val_loss: 0.5611 - val_acc: 0.8300\n",
      "Epoch 166/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4756 - acc: 0.8371 - val_loss: 0.5606 - val_acc: 0.8340\n",
      "Epoch 167/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4745 - acc: 0.8352 - val_loss: 0.5651 - val_acc: 0.8162\n",
      "Epoch 168/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4747 - acc: 0.8361 - val_loss: 0.5612 - val_acc: 0.8300\n",
      "Epoch 169/200\n",
      "1007/1007 [==============================] - 0s 104us/step - loss: 0.4743 - acc: 0.8371 - val_loss: 0.5606 - val_acc: 0.8300\n",
      "Epoch 170/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4734 - acc: 0.8361 - val_loss: 0.5649 - val_acc: 0.8340\n",
      "Epoch 171/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4739 - acc: 0.8431 - val_loss: 0.5659 - val_acc: 0.8261\n",
      "Epoch 172/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.4739 - acc: 0.8381 - val_loss: 0.5662 - val_acc: 0.8103\n",
      "Epoch 173/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4736 - acc: 0.8361 - val_loss: 0.5621 - val_acc: 0.8320\n",
      "Epoch 174/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4714 - acc: 0.8381 - val_loss: 0.5651 - val_acc: 0.8103\n",
      "Epoch 175/200\n",
      "1007/1007 [==============================] - 0s 109us/step - loss: 0.4729 - acc: 0.8381 - val_loss: 0.5625 - val_acc: 0.8261\n",
      "Epoch 176/200\n",
      "1007/1007 [==============================] - 0s 102us/step - loss: 0.4718 - acc: 0.8461 - val_loss: 0.5648 - val_acc: 0.8221\n",
      "Epoch 177/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4722 - acc: 0.8361 - val_loss: 0.5608 - val_acc: 0.8182\n",
      "Epoch 178/200\n",
      "1007/1007 [==============================] - 0s 107us/step - loss: 0.4702 - acc: 0.8371 - val_loss: 0.5658 - val_acc: 0.8202\n",
      "Epoch 179/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.4711 - acc: 0.8431 - val_loss: 0.5605 - val_acc: 0.8261\n",
      "Epoch 180/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.4703 - acc: 0.8411 - val_loss: 0.5611 - val_acc: 0.8241\n",
      "Epoch 181/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4690 - acc: 0.8342 - val_loss: 0.5645 - val_acc: 0.8300\n",
      "Epoch 182/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4706 - acc: 0.8391 - val_loss: 0.5625 - val_acc: 0.8261\n",
      "Epoch 183/200\n",
      "1007/1007 [==============================] - 0s 107us/step - loss: 0.4696 - acc: 0.8401 - val_loss: 0.5620 - val_acc: 0.8182\n",
      "Epoch 184/200\n",
      "1007/1007 [==============================] - 0s 122us/step - loss: 0.4690 - acc: 0.8361 - val_loss: 0.5610 - val_acc: 0.8241\n",
      "Epoch 185/200\n",
      "1007/1007 [==============================] - 0s 110us/step - loss: 0.4675 - acc: 0.8381 - val_loss: 0.5615 - val_acc: 0.8241\n",
      "Epoch 186/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4692 - acc: 0.8411 - val_loss: 0.5609 - val_acc: 0.8300\n",
      "Epoch 187/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.4683 - acc: 0.8421 - val_loss: 0.5605 - val_acc: 0.8261\n",
      "Epoch 188/200\n",
      "1007/1007 [==============================] - 0s 111us/step - loss: 0.4661 - acc: 0.8421 - val_loss: 0.5660 - val_acc: 0.8241\n",
      "Epoch 189/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.4663 - acc: 0.8421 - val_loss: 0.5646 - val_acc: 0.8261\n",
      "Epoch 190/200\n",
      "1007/1007 [==============================] - 0s 108us/step - loss: 0.4663 - acc: 0.8431 - val_loss: 0.5629 - val_acc: 0.8281\n",
      "Epoch 191/200\n",
      "1007/1007 [==============================] - 0s 108us/step - loss: 0.4679 - acc: 0.8391 - val_loss: 0.5611 - val_acc: 0.8300\n",
      "Epoch 192/200\n",
      "1007/1007 [==============================] - 0s 114us/step - loss: 0.4671 - acc: 0.8401 - val_loss: 0.5621 - val_acc: 0.8300\n",
      "Epoch 193/200\n",
      "1007/1007 [==============================] - 0s 116us/step - loss: 0.4658 - acc: 0.8441 - val_loss: 0.5646 - val_acc: 0.8300\n",
      "Epoch 194/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.4664 - acc: 0.8361 - val_loss: 0.5631 - val_acc: 0.8281\n",
      "Epoch 195/200\n",
      "1007/1007 [==============================] - 0s 113us/step - loss: 0.4652 - acc: 0.8411 - val_loss: 0.5659 - val_acc: 0.8300\n",
      "Epoch 196/200\n",
      "1007/1007 [==============================] - 0s 114us/step - loss: 0.4643 - acc: 0.8461 - val_loss: 0.5644 - val_acc: 0.8241\n",
      "Epoch 197/200\n",
      "1007/1007 [==============================] - 0s 118us/step - loss: 0.4656 - acc: 0.8421 - val_loss: 0.5625 - val_acc: 0.8221\n",
      "Epoch 198/200\n",
      "1007/1007 [==============================] - 0s 112us/step - loss: 0.4643 - acc: 0.8451 - val_loss: 0.5656 - val_acc: 0.8241\n",
      "Epoch 199/200\n",
      "1007/1007 [==============================] - 0s 123us/step - loss: 0.4643 - acc: 0.8401 - val_loss: 0.5648 - val_acc: 0.8281\n",
      "Epoch 200/200\n",
      "1007/1007 [==============================] - 0s 129us/step - loss: 0.4630 - acc: 0.8461 - val_loss: 0.5618 - val_acc: 0.8281\n",
      "Train on 1013 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.5282 - acc: 0.8322 - val_loss: 0.4335 - val_acc: 0.8520\n",
      "Epoch 2/200\n",
      "1013/1013 [==============================] - 0s 121us/step - loss: 0.5221 - acc: 0.8322 - val_loss: 0.4366 - val_acc: 0.8420\n",
      "Epoch 3/200\n",
      "1013/1013 [==============================] - 0s 116us/step - loss: 0.5194 - acc: 0.8272 - val_loss: 0.4399 - val_acc: 0.8480\n",
      "Epoch 4/200\n",
      "1013/1013 [==============================] - 0s 117us/step - loss: 0.5161 - acc: 0.8361 - val_loss: 0.4411 - val_acc: 0.8480\n",
      "Epoch 5/200\n",
      "1013/1013 [==============================] - 0s 115us/step - loss: 0.5109 - acc: 0.8322 - val_loss: 0.4441 - val_acc: 0.8440\n",
      "Epoch 6/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.5127 - acc: 0.8342 - val_loss: 0.4467 - val_acc: 0.8440\n",
      "Epoch 7/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.5093 - acc: 0.8381 - val_loss: 0.4479 - val_acc: 0.8460\n",
      "Epoch 8/200\n",
      "1013/1013 [==============================] - 0s 139us/step - loss: 0.5088 - acc: 0.8302 - val_loss: 0.4507 - val_acc: 0.8440\n",
      "Epoch 9/200\n",
      "1013/1013 [==============================] - 0s 139us/step - loss: 0.5072 - acc: 0.8332 - val_loss: 0.4500 - val_acc: 0.8460\n",
      "Epoch 10/200\n",
      "1013/1013 [==============================] - 0s 124us/step - loss: 0.5058 - acc: 0.8332 - val_loss: 0.4509 - val_acc: 0.8480\n",
      "Epoch 11/200\n",
      "1013/1013 [==============================] - 0s 126us/step - loss: 0.5044 - acc: 0.8312 - val_loss: 0.4515 - val_acc: 0.8460\n",
      "Epoch 12/200\n",
      "1013/1013 [==============================] - 0s 125us/step - loss: 0.5041 - acc: 0.8421 - val_loss: 0.4514 - val_acc: 0.8480\n",
      "Epoch 13/200\n",
      "1013/1013 [==============================] - 0s 128us/step - loss: 0.5016 - acc: 0.8371 - val_loss: 0.4520 - val_acc: 0.8480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.5010 - acc: 0.8371 - val_loss: 0.4526 - val_acc: 0.8440\n",
      "Epoch 15/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.5007 - acc: 0.8381 - val_loss: 0.4528 - val_acc: 0.8380\n",
      "Epoch 16/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4973 - acc: 0.8391 - val_loss: 0.4565 - val_acc: 0.8380\n",
      "Epoch 17/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4985 - acc: 0.8421 - val_loss: 0.4548 - val_acc: 0.8420\n",
      "Epoch 18/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4976 - acc: 0.8322 - val_loss: 0.4564 - val_acc: 0.8440\n",
      "Epoch 19/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4971 - acc: 0.8401 - val_loss: 0.4575 - val_acc: 0.8380\n",
      "Epoch 20/200\n",
      "1013/1013 [==============================] - 0s 101us/step - loss: 0.4960 - acc: 0.8381 - val_loss: 0.4588 - val_acc: 0.8440\n",
      "Epoch 21/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4959 - acc: 0.8421 - val_loss: 0.4619 - val_acc: 0.8400\n",
      "Epoch 22/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4954 - acc: 0.8430 - val_loss: 0.4625 - val_acc: 0.8420\n",
      "Epoch 23/200\n",
      "1013/1013 [==============================] - 0s 101us/step - loss: 0.4938 - acc: 0.8391 - val_loss: 0.4671 - val_acc: 0.8440\n",
      "Epoch 24/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4943 - acc: 0.8401 - val_loss: 0.4684 - val_acc: 0.8460\n",
      "Epoch 25/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4936 - acc: 0.8411 - val_loss: 0.4690 - val_acc: 0.8420\n",
      "Epoch 26/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4923 - acc: 0.8421 - val_loss: 0.4722 - val_acc: 0.8380\n",
      "Epoch 27/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4933 - acc: 0.8371 - val_loss: 0.4735 - val_acc: 0.8440\n",
      "Epoch 28/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4902 - acc: 0.8361 - val_loss: 0.4771 - val_acc: 0.8380\n",
      "Epoch 29/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4911 - acc: 0.8411 - val_loss: 0.4798 - val_acc: 0.8360\n",
      "Epoch 30/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4901 - acc: 0.8460 - val_loss: 0.4821 - val_acc: 0.8400\n",
      "Epoch 31/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4902 - acc: 0.8460 - val_loss: 0.4813 - val_acc: 0.8360\n",
      "Epoch 32/200\n",
      "1013/1013 [==============================] - 0s 103us/step - loss: 0.4896 - acc: 0.8411 - val_loss: 0.4846 - val_acc: 0.8360\n",
      "Epoch 33/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4888 - acc: 0.8421 - val_loss: 0.4881 - val_acc: 0.8380\n",
      "Epoch 34/200\n",
      "1013/1013 [==============================] - 0s 102us/step - loss: 0.4882 - acc: 0.8401 - val_loss: 0.4914 - val_acc: 0.8360\n",
      "Epoch 35/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4877 - acc: 0.8421 - val_loss: 0.4910 - val_acc: 0.8340\n",
      "Epoch 36/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4873 - acc: 0.8401 - val_loss: 0.4906 - val_acc: 0.8360\n",
      "Epoch 37/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4864 - acc: 0.8401 - val_loss: 0.4902 - val_acc: 0.8380\n",
      "Epoch 38/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4847 - acc: 0.8371 - val_loss: 0.4899 - val_acc: 0.8380\n",
      "Epoch 39/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4846 - acc: 0.8421 - val_loss: 0.4899 - val_acc: 0.8380\n",
      "Epoch 40/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4842 - acc: 0.8381 - val_loss: 0.4906 - val_acc: 0.8380\n",
      "Epoch 41/200\n",
      "1013/1013 [==============================] - 0s 124us/step - loss: 0.4853 - acc: 0.8430 - val_loss: 0.4909 - val_acc: 0.8380\n",
      "Epoch 42/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4851 - acc: 0.8470 - val_loss: 0.4925 - val_acc: 0.8420\n",
      "Epoch 43/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4843 - acc: 0.8401 - val_loss: 0.4934 - val_acc: 0.8340\n",
      "Epoch 44/200\n",
      "1013/1013 [==============================] - 0s 92us/step - loss: 0.4837 - acc: 0.8401 - val_loss: 0.4920 - val_acc: 0.8360\n",
      "Epoch 45/200\n",
      "1013/1013 [==============================] - 0s 128us/step - loss: 0.4828 - acc: 0.8440 - val_loss: 0.4928 - val_acc: 0.8360\n",
      "Epoch 46/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4821 - acc: 0.8430 - val_loss: 0.4956 - val_acc: 0.8280\n",
      "Epoch 47/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4818 - acc: 0.8342 - val_loss: 0.4972 - val_acc: 0.8360\n",
      "Epoch 48/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4823 - acc: 0.8440 - val_loss: 0.4964 - val_acc: 0.8380\n",
      "Epoch 49/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4816 - acc: 0.8430 - val_loss: 0.4960 - val_acc: 0.8380\n",
      "Epoch 50/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4821 - acc: 0.8411 - val_loss: 0.4951 - val_acc: 0.8360\n",
      "Epoch 51/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4805 - acc: 0.8411 - val_loss: 0.4944 - val_acc: 0.8380\n",
      "Epoch 52/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4804 - acc: 0.8411 - val_loss: 0.4956 - val_acc: 0.8360\n",
      "Epoch 53/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4796 - acc: 0.8450 - val_loss: 0.4955 - val_acc: 0.8380\n",
      "Epoch 54/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4784 - acc: 0.8440 - val_loss: 0.4969 - val_acc: 0.8300\n",
      "Epoch 55/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4785 - acc: 0.8430 - val_loss: 0.4964 - val_acc: 0.8340\n",
      "Epoch 56/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4793 - acc: 0.8500 - val_loss: 0.4960 - val_acc: 0.8340\n",
      "Epoch 57/200\n",
      "1013/1013 [==============================] - 0s 115us/step - loss: 0.4786 - acc: 0.8509 - val_loss: 0.4973 - val_acc: 0.8340\n",
      "Epoch 58/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4782 - acc: 0.8430 - val_loss: 0.4973 - val_acc: 0.8360\n",
      "Epoch 59/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4774 - acc: 0.8460 - val_loss: 0.4969 - val_acc: 0.8360\n",
      "Epoch 60/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4766 - acc: 0.8470 - val_loss: 0.4969 - val_acc: 0.8340\n",
      "Epoch 61/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4750 - acc: 0.8430 - val_loss: 0.5003 - val_acc: 0.8340\n",
      "Epoch 62/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4758 - acc: 0.8440 - val_loss: 0.4964 - val_acc: 0.8360\n",
      "Epoch 63/200\n",
      "1013/1013 [==============================] - 0s 117us/step - loss: 0.4759 - acc: 0.8440 - val_loss: 0.4974 - val_acc: 0.8320\n",
      "Epoch 64/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4735 - acc: 0.8519 - val_loss: 0.4994 - val_acc: 0.8340\n",
      "Epoch 65/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4746 - acc: 0.8411 - val_loss: 0.4998 - val_acc: 0.8360\n",
      "Epoch 66/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4741 - acc: 0.8460 - val_loss: 0.4993 - val_acc: 0.8300\n",
      "Epoch 67/200\n",
      "1013/1013 [==============================] - 0s 115us/step - loss: 0.4724 - acc: 0.8450 - val_loss: 0.5015 - val_acc: 0.8420\n",
      "Epoch 68/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4741 - acc: 0.8480 - val_loss: 0.4985 - val_acc: 0.8320\n",
      "Epoch 69/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4733 - acc: 0.8460 - val_loss: 0.4991 - val_acc: 0.8300\n",
      "Epoch 70/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4726 - acc: 0.8500 - val_loss: 0.5007 - val_acc: 0.8300\n",
      "Epoch 71/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4728 - acc: 0.8460 - val_loss: 0.4985 - val_acc: 0.8320\n",
      "Epoch 72/200\n",
      "1013/1013 [==============================] - 0s 115us/step - loss: 0.4713 - acc: 0.8470 - val_loss: 0.5002 - val_acc: 0.8340\n",
      "Epoch 73/200\n",
      "1013/1013 [==============================] - 0s 126us/step - loss: 0.4712 - acc: 0.8460 - val_loss: 0.5003 - val_acc: 0.8320\n",
      "Epoch 74/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4708 - acc: 0.8480 - val_loss: 0.5017 - val_acc: 0.8360\n",
      "Epoch 75/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4701 - acc: 0.8460 - val_loss: 0.5012 - val_acc: 0.8320\n",
      "Epoch 76/200\n",
      "1013/1013 [==============================] - 0s 102us/step - loss: 0.4709 - acc: 0.8480 - val_loss: 0.5006 - val_acc: 0.8320\n",
      "Epoch 77/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4691 - acc: 0.8460 - val_loss: 0.5014 - val_acc: 0.8280\n",
      "Epoch 78/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4693 - acc: 0.8500 - val_loss: 0.4999 - val_acc: 0.8280\n",
      "Epoch 79/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4681 - acc: 0.8480 - val_loss: 0.5037 - val_acc: 0.8280\n",
      "Epoch 80/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4701 - acc: 0.8500 - val_loss: 0.5010 - val_acc: 0.8320\n",
      "Epoch 81/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4672 - acc: 0.8529 - val_loss: 0.5057 - val_acc: 0.8360\n",
      "Epoch 82/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4691 - acc: 0.8500 - val_loss: 0.5019 - val_acc: 0.8340\n",
      "Epoch 83/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4686 - acc: 0.8480 - val_loss: 0.5025 - val_acc: 0.8360\n",
      "Epoch 84/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4673 - acc: 0.8500 - val_loss: 0.5042 - val_acc: 0.8340\n",
      "Epoch 85/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4669 - acc: 0.8470 - val_loss: 0.5044 - val_acc: 0.8300\n",
      "Epoch 86/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4669 - acc: 0.8490 - val_loss: 0.5039 - val_acc: 0.8340\n",
      "Epoch 87/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4669 - acc: 0.8500 - val_loss: 0.5055 - val_acc: 0.8320\n",
      "Epoch 88/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4645 - acc: 0.8470 - val_loss: 0.5053 - val_acc: 0.8280\n",
      "Epoch 89/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4654 - acc: 0.8509 - val_loss: 0.5054 - val_acc: 0.8320\n",
      "Epoch 90/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4654 - acc: 0.8500 - val_loss: 0.5056 - val_acc: 0.8300\n",
      "Epoch 91/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4649 - acc: 0.8519 - val_loss: 0.5063 - val_acc: 0.8280\n",
      "Epoch 92/200\n",
      "1013/1013 [==============================] - 0s 102us/step - loss: 0.4650 - acc: 0.8500 - val_loss: 0.5064 - val_acc: 0.8300\n",
      "Epoch 93/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4645 - acc: 0.8470 - val_loss: 0.5068 - val_acc: 0.8320\n",
      "Epoch 94/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4653 - acc: 0.8509 - val_loss: 0.5080 - val_acc: 0.8400\n",
      "Epoch 95/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4633 - acc: 0.8490 - val_loss: 0.5083 - val_acc: 0.8360\n",
      "Epoch 96/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4639 - acc: 0.8450 - val_loss: 0.5086 - val_acc: 0.8360\n",
      "Epoch 97/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4628 - acc: 0.8470 - val_loss: 0.5096 - val_acc: 0.8360\n",
      "Epoch 98/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4636 - acc: 0.8490 - val_loss: 0.5093 - val_acc: 0.8280\n",
      "Epoch 99/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4619 - acc: 0.8529 - val_loss: 0.5103 - val_acc: 0.8520\n",
      "Epoch 100/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4630 - acc: 0.8500 - val_loss: 0.5125 - val_acc: 0.8400\n",
      "Epoch 101/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4623 - acc: 0.8460 - val_loss: 0.5112 - val_acc: 0.8300\n",
      "Epoch 102/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4616 - acc: 0.8490 - val_loss: 0.5112 - val_acc: 0.8320\n",
      "Epoch 103/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4614 - acc: 0.8519 - val_loss: 0.5127 - val_acc: 0.8360\n",
      "Epoch 104/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4611 - acc: 0.8519 - val_loss: 0.5141 - val_acc: 0.8280\n",
      "Epoch 105/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4612 - acc: 0.8519 - val_loss: 0.5152 - val_acc: 0.8380\n",
      "Epoch 106/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4587 - acc: 0.8529 - val_loss: 0.5195 - val_acc: 0.8340\n",
      "Epoch 107/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4597 - acc: 0.8539 - val_loss: 0.5200 - val_acc: 0.8360\n",
      "Epoch 108/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4618 - acc: 0.8529 - val_loss: 0.5178 - val_acc: 0.8360\n",
      "Epoch 109/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4604 - acc: 0.8529 - val_loss: 0.5167 - val_acc: 0.8340\n",
      "Epoch 110/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4600 - acc: 0.8549 - val_loss: 0.5207 - val_acc: 0.8460\n",
      "Epoch 111/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4593 - acc: 0.8470 - val_loss: 0.5204 - val_acc: 0.8380\n",
      "Epoch 112/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4599 - acc: 0.8480 - val_loss: 0.5185 - val_acc: 0.8360\n",
      "Epoch 113/200\n",
      "1013/1013 [==============================] - 0s 103us/step - loss: 0.4593 - acc: 0.8480 - val_loss: 0.5198 - val_acc: 0.8340\n",
      "Epoch 114/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4574 - acc: 0.8490 - val_loss: 0.5229 - val_acc: 0.8380\n",
      "Epoch 115/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4601 - acc: 0.8490 - val_loss: 0.5186 - val_acc: 0.8340\n",
      "Epoch 116/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4576 - acc: 0.8480 - val_loss: 0.5179 - val_acc: 0.8360\n",
      "Epoch 117/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4581 - acc: 0.8529 - val_loss: 0.5182 - val_acc: 0.8340\n",
      "Epoch 118/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4568 - acc: 0.8480 - val_loss: 0.5204 - val_acc: 0.8340\n",
      "Epoch 119/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4578 - acc: 0.8490 - val_loss: 0.5220 - val_acc: 0.8360\n",
      "Epoch 120/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4566 - acc: 0.8470 - val_loss: 0.5194 - val_acc: 0.8380\n",
      "Epoch 121/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4573 - acc: 0.8460 - val_loss: 0.5182 - val_acc: 0.8400\n",
      "Epoch 122/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4560 - acc: 0.8509 - val_loss: 0.5210 - val_acc: 0.8320\n",
      "Epoch 123/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4558 - acc: 0.8490 - val_loss: 0.5243 - val_acc: 0.8260\n",
      "Epoch 124/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4554 - acc: 0.8539 - val_loss: 0.5201 - val_acc: 0.8300\n",
      "Epoch 125/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4567 - acc: 0.8569 - val_loss: 0.5224 - val_acc: 0.8320\n",
      "Epoch 126/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4543 - acc: 0.8519 - val_loss: 0.5220 - val_acc: 0.8420\n",
      "Epoch 127/200\n",
      "1013/1013 [==============================] - 0s 122us/step - loss: 0.4560 - acc: 0.8529 - val_loss: 0.5230 - val_acc: 0.8340\n",
      "Epoch 128/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4551 - acc: 0.8539 - val_loss: 0.5232 - val_acc: 0.8340\n",
      "Epoch 129/200\n",
      "1013/1013 [==============================] - 0s 118us/step - loss: 0.4548 - acc: 0.8519 - val_loss: 0.5237 - val_acc: 0.8260\n",
      "Epoch 130/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4548 - acc: 0.8539 - val_loss: 0.5231 - val_acc: 0.8400\n",
      "Epoch 131/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4542 - acc: 0.8490 - val_loss: 0.5263 - val_acc: 0.8240\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4543 - acc: 0.8598 - val_loss: 0.5234 - val_acc: 0.8420\n",
      "Epoch 133/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4542 - acc: 0.8588 - val_loss: 0.5238 - val_acc: 0.8320\n",
      "Epoch 134/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4531 - acc: 0.8588 - val_loss: 0.5255 - val_acc: 0.8340\n",
      "Epoch 135/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4534 - acc: 0.8529 - val_loss: 0.5256 - val_acc: 0.8340\n",
      "Epoch 136/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4526 - acc: 0.8559 - val_loss: 0.5266 - val_acc: 0.8360\n",
      "Epoch 137/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4531 - acc: 0.8529 - val_loss: 0.5240 - val_acc: 0.8400\n",
      "Epoch 138/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4530 - acc: 0.8559 - val_loss: 0.5268 - val_acc: 0.8380\n",
      "Epoch 139/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4528 - acc: 0.8588 - val_loss: 0.5268 - val_acc: 0.8280\n",
      "Epoch 140/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4523 - acc: 0.8440 - val_loss: 0.5261 - val_acc: 0.8360\n",
      "Epoch 141/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4511 - acc: 0.8598 - val_loss: 0.5283 - val_acc: 0.8400\n",
      "Epoch 142/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4523 - acc: 0.8559 - val_loss: 0.5283 - val_acc: 0.8360\n",
      "Epoch 143/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4507 - acc: 0.8500 - val_loss: 0.5269 - val_acc: 0.8400\n",
      "Epoch 144/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4515 - acc: 0.8539 - val_loss: 0.5277 - val_acc: 0.8400\n",
      "Epoch 145/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4509 - acc: 0.8529 - val_loss: 0.5270 - val_acc: 0.8340\n",
      "Epoch 146/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4516 - acc: 0.8559 - val_loss: 0.5284 - val_acc: 0.8440\n",
      "Epoch 147/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4508 - acc: 0.8529 - val_loss: 0.5282 - val_acc: 0.8440\n",
      "Epoch 148/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4510 - acc: 0.8539 - val_loss: 0.5283 - val_acc: 0.8420\n",
      "Epoch 149/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4494 - acc: 0.8509 - val_loss: 0.5305 - val_acc: 0.8360\n",
      "Epoch 150/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4504 - acc: 0.8578 - val_loss: 0.5289 - val_acc: 0.8420\n",
      "Epoch 151/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4501 - acc: 0.8598 - val_loss: 0.5300 - val_acc: 0.8380\n",
      "Epoch 152/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4497 - acc: 0.8549 - val_loss: 0.5306 - val_acc: 0.8300\n",
      "Epoch 153/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4499 - acc: 0.8569 - val_loss: 0.5298 - val_acc: 0.8320\n",
      "Epoch 154/200\n",
      "1013/1013 [==============================] - 0s 102us/step - loss: 0.4497 - acc: 0.8569 - val_loss: 0.5289 - val_acc: 0.8380\n",
      "Epoch 155/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4488 - acc: 0.8539 - val_loss: 0.5299 - val_acc: 0.8360\n",
      "Epoch 156/200\n",
      "1013/1013 [==============================] - 0s 102us/step - loss: 0.4483 - acc: 0.8578 - val_loss: 0.5310 - val_acc: 0.8440\n",
      "Epoch 157/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4478 - acc: 0.8549 - val_loss: 0.5292 - val_acc: 0.8340\n",
      "Epoch 158/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4486 - acc: 0.8588 - val_loss: 0.5294 - val_acc: 0.8400\n",
      "Epoch 159/200\n",
      "1013/1013 [==============================] - 0s 122us/step - loss: 0.4479 - acc: 0.8569 - val_loss: 0.5297 - val_acc: 0.8400\n",
      "Epoch 160/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4475 - acc: 0.8559 - val_loss: 0.5327 - val_acc: 0.8440\n",
      "Epoch 161/200\n",
      "1013/1013 [==============================] - 0s 94us/step - loss: 0.4474 - acc: 0.8549 - val_loss: 0.5330 - val_acc: 0.8260\n",
      "Epoch 162/200\n",
      "1013/1013 [==============================] - 0s 128us/step - loss: 0.4478 - acc: 0.8529 - val_loss: 0.5354 - val_acc: 0.8300\n",
      "Epoch 163/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4470 - acc: 0.8578 - val_loss: 0.5339 - val_acc: 0.8340\n",
      "Epoch 164/200\n",
      "1013/1013 [==============================] - 0s 115us/step - loss: 0.4469 - acc: 0.8559 - val_loss: 0.5347 - val_acc: 0.8260\n",
      "Epoch 165/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4469 - acc: 0.8559 - val_loss: 0.5356 - val_acc: 0.8340\n",
      "Epoch 166/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4457 - acc: 0.8569 - val_loss: 0.5390 - val_acc: 0.8360\n",
      "Epoch 167/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4462 - acc: 0.8529 - val_loss: 0.5351 - val_acc: 0.8300\n",
      "Epoch 168/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4459 - acc: 0.8578 - val_loss: 0.5347 - val_acc: 0.8300\n",
      "Epoch 169/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4467 - acc: 0.8608 - val_loss: 0.5356 - val_acc: 0.8300\n",
      "Epoch 170/200\n",
      "1013/1013 [==============================] - 0s 121us/step - loss: 0.4448 - acc: 0.8569 - val_loss: 0.5371 - val_acc: 0.8320\n",
      "Epoch 171/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4456 - acc: 0.8578 - val_loss: 0.5367 - val_acc: 0.8300\n",
      "Epoch 172/200\n",
      "1013/1013 [==============================] - 0s 122us/step - loss: 0.4449 - acc: 0.8529 - val_loss: 0.5367 - val_acc: 0.8360\n",
      "Epoch 173/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4451 - acc: 0.8628 - val_loss: 0.5372 - val_acc: 0.8300\n",
      "Epoch 174/200\n",
      "1013/1013 [==============================] - 0s 127us/step - loss: 0.4453 - acc: 0.8588 - val_loss: 0.5367 - val_acc: 0.8340\n",
      "Epoch 175/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4445 - acc: 0.8578 - val_loss: 0.5366 - val_acc: 0.8380\n",
      "Epoch 176/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4451 - acc: 0.8559 - val_loss: 0.5354 - val_acc: 0.8380\n",
      "Epoch 177/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4433 - acc: 0.8490 - val_loss: 0.5385 - val_acc: 0.8360\n",
      "Epoch 178/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4441 - acc: 0.8559 - val_loss: 0.5387 - val_acc: 0.8380\n",
      "Epoch 179/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4443 - acc: 0.8569 - val_loss: 0.5385 - val_acc: 0.8360\n",
      "Epoch 180/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4434 - acc: 0.8549 - val_loss: 0.5361 - val_acc: 0.8360\n",
      "Epoch 181/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4439 - acc: 0.8539 - val_loss: 0.5375 - val_acc: 0.8360\n",
      "Epoch 182/200\n",
      "1013/1013 [==============================] - 0s 112us/step - loss: 0.4422 - acc: 0.8598 - val_loss: 0.5375 - val_acc: 0.8380\n",
      "Epoch 183/200\n",
      "1013/1013 [==============================] - 0s 109us/step - loss: 0.4432 - acc: 0.8578 - val_loss: 0.5385 - val_acc: 0.8440\n",
      "Epoch 184/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4430 - acc: 0.8509 - val_loss: 0.5391 - val_acc: 0.8300\n",
      "Epoch 185/200\n",
      "1013/1013 [==============================] - 0s 120us/step - loss: 0.4426 - acc: 0.8638 - val_loss: 0.5387 - val_acc: 0.8320\n",
      "Epoch 186/200\n",
      "1013/1013 [==============================] - 0s 111us/step - loss: 0.4426 - acc: 0.8509 - val_loss: 0.5375 - val_acc: 0.8360\n",
      "Epoch 187/200\n",
      "1013/1013 [==============================] - 0s 123us/step - loss: 0.4425 - acc: 0.8559 - val_loss: 0.5380 - val_acc: 0.8320\n",
      "Epoch 188/200\n",
      "1013/1013 [==============================] - 0s 117us/step - loss: 0.4418 - acc: 0.8549 - val_loss: 0.5389 - val_acc: 0.8420\n",
      "Epoch 189/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4386 - acc: 0.8559 - val_loss: 0.5391 - val_acc: 0.8400\n",
      "Epoch 190/200\n",
      "1013/1013 [==============================] - 0s 113us/step - loss: 0.4424 - acc: 0.8559 - val_loss: 0.5376 - val_acc: 0.8300\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013/1013 [==============================] - 0s 124us/step - loss: 0.4411 - acc: 0.8559 - val_loss: 0.5388 - val_acc: 0.8420\n",
      "Epoch 192/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4416 - acc: 0.8598 - val_loss: 0.5372 - val_acc: 0.8400\n",
      "Epoch 193/200\n",
      "1013/1013 [==============================] - 0s 110us/step - loss: 0.4399 - acc: 0.8598 - val_loss: 0.5392 - val_acc: 0.8380\n",
      "Epoch 194/200\n",
      "1013/1013 [==============================] - 0s 104us/step - loss: 0.4406 - acc: 0.8539 - val_loss: 0.5396 - val_acc: 0.8460\n",
      "Epoch 195/200\n",
      "1013/1013 [==============================] - 0s 114us/step - loss: 0.4423 - acc: 0.8539 - val_loss: 0.5413 - val_acc: 0.8400\n",
      "Epoch 196/200\n",
      "1013/1013 [==============================] - 0s 105us/step - loss: 0.4401 - acc: 0.8608 - val_loss: 0.5423 - val_acc: 0.8340\n",
      "Epoch 197/200\n",
      "1013/1013 [==============================] - 0s 106us/step - loss: 0.4401 - acc: 0.8529 - val_loss: 0.5406 - val_acc: 0.8360\n",
      "Epoch 198/200\n",
      "1013/1013 [==============================] - 0s 108us/step - loss: 0.4396 - acc: 0.8559 - val_loss: 0.5416 - val_acc: 0.8360\n",
      "Epoch 199/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4405 - acc: 0.8549 - val_loss: 0.5409 - val_acc: 0.8380\n",
      "Epoch 200/200\n",
      "1013/1013 [==============================] - 0s 107us/step - loss: 0.4388 - acc: 0.8559 - val_loss: 0.5430 - val_acc: 0.8320\n"
     ]
    }
   ],
   "source": [
    "k_fold.cross_validate(data, label, sensor_num=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'k_fold' from 'C:\\\\Users\\\\Jay\\\\Desktop\\\\capstone\\\\k_fold.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-492415a718af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'read'"
     ]
    }
   ],
   "source": [
    "import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-39304553e64d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-39304553e64d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    import module/read\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import module/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2963\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-8370c5abf131>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    exec(\"/modules/read.py\")\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    /modules/read.py\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "exec(\"/modules/read.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"modules/read.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-492415a718af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'read'"
     ]
    }
   ],
   "source": [
    "import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../read.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-492415a718af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'read'"
     ]
    }
   ],
   "source": [
    "import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = read.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'read' from 'C:\\\\Users\\\\Jay\\\\Desktop\\\\capstone\\\\Neural-Network-Classifier\\\\read.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 5 2 1 5 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
